\documentclass[12pt, a4paper]{article}
\usepackage[ascii]{inputenc}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=4cm]{geometry}
\usepackage[protrusion=true,expansion=true]{microtype}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz, pgfplots}
\usepackage{kpfonts}
\pgfplotsset{compat=1.13}

\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\C}{\mathbb{C}}
\DeclareMathOperator{\F}{\mathbb{F}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\re}{re}
\DeclareMathOperator{\proj}{proj}
\renewcommand{\Pr}{\mathbb{P}}

\usepackage{graphicx}
\usepackage{enumitem}
\setenumerate[1]{label=\textbf{\arabic*.}}

%-----------------------------------------------------------------------------------------------------------------
% Some fancy macros // May eventually move these into separate files or something and merge when building template
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}} % dx macro for integrals
\newcommand{\hess}[1]{\ensuremath{\operatorname{H}\!{#1}}} % Hessian
\newcommand{\diff}[1]{\ensuremath{\operatorname{D}\!{#1}}} % Jacobian
\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle} % inner product
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % norm
\newcommand{\cpl}[1]{\overline{#1}} % complement
\renewcommand{\v}[1]{\mathbf{#1}} % vector
\newenvironment{amatrix}[1]{% augumented matrix - make sure to have # columns less than required amount
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
%-----------------------------------------------------------------------------------------------------------------
% Define theorem environments, along with a custom proof environment
\usepackage[thref, thmmarks,amsmath]{ntheorem}
\newcommand{\itref}[1]{\textit{\thref{#1}}}

\theoremseparator{}
\theoremindent0.0cm
\theoremstyle{nonumberplain}
\theoremheaderfont{\scshape}
\theorembodyfont{\upshape}
\theoremsymbol{$//$}
\newtheorem{solution}{Sol'n}
\theoremsymbol{$\blacksquare$}
\newtheorem{proof}{Proof}

%-----------------------------------------------------------------------------------------------------------------
% Define Document Variables
\newcommand{\assignmentname}{Study Notes}
\newcommand{\classname}{Introduction to Probability}
\newcommand{\duedate}{Fall 2018}

% Define a title page for the document
%-----------------------------------------------------------------------------------------------------------------
% Define headings for each page
\usepackage{fancyheadings}
\pagestyle{fancy}
\lhead{Alex Rutar\\ID 2065 1307}
\chead{\classname}
\rhead{\assignmentname\\\duedate}
\cfoot{\thepage}
\setlength{\headheight}{50pt}
%-----------------------------------------------------------------------------------------------------------------
\begin{document}
\begin{enumerate}
    \item A \textbf{probability space} is a measurable space $(S,\mathcal{M},\mu)$ with $\mathcal{M}\subseteq\mathcal{P}(S)$ and $\mu:\mathcal{M}\to[0,1]$ with $\mu(S=1$.
        In particular, we will write $\mathcal{M}=\mathcal{F}$, $\Omega=S$ and $\mu=\Pr$.
        The sets $E\in\mathcal{M}$ are called \textbf{events}.
        The $\sigma-$algebra $\mathcal{F}$ must satisfy the following axioms:
        \begin{enumerate}
            \item $\emptyset\in\mathcal{F}$
            \item If $A\in\mathcal{F}$, then $A^c\in\mathcal{F}$
            \item If $A_1,A_2,\ldots\in\mathcal{F}$, then $\bigcup\limits_{i=1}^\infty A_i\in\mathcal{F}$.
        \end{enumerate}
        As well, $\Pr$ must satisfy
        \begin{enumerate}
            \item $\Pr(A)\geq 0$ for all $A\in\mathcal{F}$.
            \item $\Pr(\Omega)=1$.
            \item Let $A_1,A_2,\ldots$ be disjoint.
                Then $\Pr\left(\bigcup\limits_{i=1}^\infty A_i\right)=\sum\limits_{i=1}^\infty \Pr(A_i)$.
        \end{enumerate}
        As a result, we collect some general properties here:
        \begin{enumerate}
            \item $\Pr(A^c)+\Pr(A)=1$
            \item $\displaystyle\Pr\left(\bigcup\limits_{i=1}^n A_i\right)=\sum\limits_{r=1}^n(-1)^{r+1}\sum\limits_{1\leq i_1<\cdots<i_r\leq n}\Pr(A_{i_1}\cap\cdots\cap A_{i_r})$
            \item We say $D_1,D_2,\ldots$ is a \textbf{decreasing} (resp. \textbf{increasing}) sequence of events if $D_1\supset D_2\cdots$ (resp. $D_1\subset D_2\cdots$) and define $\lim_{n\to\infty}D_n=\bigcap\limits_{i=1}^\infty D_n$ (resp. $\lim_{n\to\infty}D_n=\bigcup\limits_{i=1}^\infty D_n)$.
                Then $\Pr\left(\lim_{n\to\infty}D_n\right)=\lim_{n\to\infty}\Pr(D_n)$.
        \end{enumerate}
    \item Let $E,F\in\mathcal{F}$.
        Fix $E$ with $\Pr(E)>0$.
        Then define $\Pr(F|E)=\frac{\Pr(F\cap E)}{\Pr(E)}$, the \textbf{conditional probability} of $F$ given $E$.
        In fact, the map $\Pr(\cdot|E):\mathcal{F}\to\R$ is a probability measure in its own right.
        Conditional probability has some nice properties:
        \begin{enumerate}
            \item $\Pr(A_1\cap A_2\cap\cdots\cap A_n)=\Pr(A_1|A_2\cap\cdots A_n)\cdots\Pr(A_3|A_2\cap A_1)\Pr(A_2|A_1)\Pr(A_1)$.
            \item Let $A_1,A_2,\ldots,A_n\in\mathcal{F}$ be a partition of $\Omega$.
                Then for any $B\in\mathcal{F}$,
                \[\Pr(B)=\sum\limits_{i=1}^n\Pr(B|A_i)\Pr(A_i)\]
                and
                \[\Pr(A_k|B)=\frac{\Pr(B|A_k)\Pr(A_k)}{\sum\limits_{i=1}^n\Pr(B|A_i)\Pr(A_i)}\]
        \end{enumerate}
    \item Let $A_1,A_2,\ldots,A_n\in\mathcal{F}$.
        We say the events are \textbf{independent (as a collection)} if
        \[\Pr(A_{i_1}\cap\cdots\cap A_{i_k})=\Pr(A_{i_1})\cdots\Pr(A_{i_k})\]
        We say the evens are \textbf{conditionally independent} with respect to $B$ if they are independent with respect to the conditional probability measure:
        \[\Pr(A_{i_1}\cap\cdots\cap A_{i_k}|B)=\Pr(A_{i_1}|B)\cdots\Pr(A_{i_k}|B)\]
    \item A \textbf{random variable} is a measurable function $f:S\to\R$.
        We usually write random variables with capital letters, like $X$.
        We say $X$ is \textbf{discrete} if its range is countable.

        Let $\mathcal{B}$ denote the Borel measurable subset of $\R$.
        The \textbf{distribution} of a measurable function is a measure $\rho:\mathcal{B}\to\R$ given by $\rho(S)=\Pr(f^{-1}(S))$.
        This is indeed measurable: by definition, $X$ is measurable if it pulls open sets in $\R$ to elements of $\mathcal{F}$.

        For some reason, probabilists dislike writing sets down properly.
        We say $\Pr(X=i)=\Pr(X^{-1}(i))$, $\Pr(X\leq i)=\Pr(X^{-1}(-\infty,i]))$, and other similar notations.

        The \textbf{expected value} of a random variable $X$ is given by $\E(X)=\int_\Omega X\d{\Pr}$.
        When $X$ is discrete and $X(\Omega)=\{x_1,x_2,\ldots\}$, this becomes $\E(X)=\sum\limits_{k=1}^\infty x_k\Pr(X=x_k)$.
        Note that the sum need not necessarily exist.
        \begin{enumerate}
            \item If $X$ is discrete and $g:\R\to\R$, then $\E(g(X))=\sum\limits_{k=1}^\infty g(x_k)\Pr(X=x_k)$.
            \item $\E(aX+Y)=a\E(X)+\E(Y)$.
        \end{enumerate}
        We define the \textbf{variance} $\Var(X)=\inf_{a\in\Omega}\E[(X-a)^2]$.
        If $X\in L^2$, then this value is minimized for $a=\E(X)$ and $\Var(X)=\E[(X-\E(X))^2]$.
        \begin{enumerate}
            \item $\Var(X)=\E(X^2)-\E(X)^2$
            \item $\Var(aX+b)=a^2\Var(X)$
        \end{enumerate}
    \item We say that a discrete random variable is \textbf{Poisson}, i.e. $X\sim\operatorname{Poi}(\lambda)$ if it has image $\{0,1,2,\ldots\}$ and density function
        \[\Pr(X=k)=e^{-\lambda}\frac{\lambda^k}{k!}\]
        Poisson random variables arise from Binomial distributions, taking the limit as $n\to\infty$ and $p\to\0$ so that $np\to\lambda$.
        They also arise as a \textbf{Poisson process} with intensity $\lambda$ if there is a point process such that for any interval $I$ with length $h$, the probability that there is exactly one point in $I$ is $\lambda h$ (with perhaps some sublinear additional terms).
        If a point process satisfies the above conditions, $I_t$ is an arbitrary interval with length $t$, and $N_t$ counts the number of impacts with $I_t$, then $N_t\sim\operatorname{Poi}(\lambda t)$.
        A sum of Poisson random variables $X_i$ with parameters $\lamda_i$ is Poisson with paramaters $\sum_{i=1}^n\lambda_i$.
    \item Let $(\Omega,\mathcal{M},\Pr)$ be a measure space, and let $X:\Omega\to\R$ be a random variable.
        Then define $P_X$ on $\R$ by $P_X(E)=\Pr(X^{-1}(E))$, which is a Borel measure on $\R$ called the \textbf{distribution} of $X$.
        Then the function
        \[F(t)=P_X((-\infty,t])\] %)
        is called the \textbf{(cumulative) distribution function} of $X$.
        A family of functions $\{X_\alpha\}_{\alpha\in A}$ is \textbf{identically distributed} if $P_{X_\alpha}=P_{X_\beta}$ for any $\alpha,\beta\in A$.
        In the special case $\Omega=\R$ and $\Pr$ is the standard Lebesgue measure on $\R$, $X:\R\to\R$ is a Lebesgue measureable function.
        There is a particularly nice class of random variables called \textbf{absolutely continuous} random variables, in which the measure $P_X$ has a nice form.
        We say $X$ is absolutely continuous if there exists some $f_X:\R\to\R$ so that for any $E\in\mathcal{B}$,
        \[P_X(E)=\int_E f_X(x)\d{x}\]
        In the absolutely continuous case, things are particularly nice.
        If $g$ is any measurable function and $X$ is a random variable,
        \[\E(g(X))=\int_{-\infty}^\infty g(x)f_X(x)\d{x}\]
        and the case $g(x)=x$ gives the standard way to compute the expected value of a random variable.

        Suppose $k$ is a strictly increasing function, $X$ is an absolutely continuous random variable with density $f_X$.
        Let $Y=k(X)$; then we have
        \begin{equation*} f_Y(x) = f_X(k^{-1}(x))\frac{\d{}}{\d{x}}k^{-1}(x)=\frac{f_X(k^{-1}(x))}{k'(k^{-1}(x))}\end{equation*}
        Given random variables $X$ and $Y$, we can compute
        \[F_{X+Y}(a)=\int_{-\infty}^\infty F_X(a-y)f_Y(y)\d{y}\]
        which is the \textbf{convolution} of the random variables $X$ and $Y$.
    \item We say that $\phi$ is a \textbf{standard normal distribution} on $\R$ if it has density
        \[\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\]
        and we denote its CDF by $\Phi(x)$.
        In this standard form, we have $\E(X)=0$ and $\Var(X)=1$.
        More generally, we say $X\sim\mathcal{N}(\mu,\sigma^2)$ if $X=\sigma Z+\mu$ where $Z\sim\mathcal{N}(0,1)$.
        If $X\sim\mathcal{N}(\mu_1,\sigma_1^2)$ and $Y\sim\mathcal{N}(\mu_2,\sigma_2^2)$, then
        \[X+aY\sim\mathcal{N}(\mu_1+a\mu_2,\sigma_1^2+a^2\sigma_2^2)\]

    \item If we have a collection of random variables $(X_1,\ldots,X_n)$ then we can consider them as a map $(X_1,\ldots,X_n):\Omega\to\R^n$, and the corresponding measure is called the \textbf{joint distribution} of $X_1,\ldots,X_n$.
        Often, we take $\Omega=\R^n$ and $\Pr$ to be the standard Lebesgue measure on $\R^n$.
    \item 
    \item We say that random variables $X,Y$ are \textbf{independent} if
        \[\Pr(X\in A,Y\in B)=\Pr(X\in A)\Pr(Y\in B)\]
        for any Borel sets $A,B$.
        Equivalently, we require
        \[F_{X,Y}(a,b)=F_X(a)F_X(b)\]
\end{enumerate}
\end{document}
