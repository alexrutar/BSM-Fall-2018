\documentclass[12pt, a4paper]{book}
\usepackage[ascii]{inputenc}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=4cm]{geometry}
\usepackage[protrusion=true,expansion=true]{microtype}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz, pgfplots}
\usetikzlibrary{intersections}
\usepackage{kpfonts}
\usepackage{dsfont}
\pgfplotsset{compat=1.13}
\usepackage{emptypage}

\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\C}{\mathbb{C}}
\DeclareMathOperator{\F}{\mathbb{F}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Binom}{Binom}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Poi}{Poi}
\DeclareMathOperator{\UNI}{UNI}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\sgn}{sgn}
\renewcommand{\Pr}{\mathbb{P}}

\usepackage{graphicx}
\usepackage{enumitem}
\setenumerate{}

%-----------------------------------------------------------------------------------------------------------------
% Some fancy macros // May eventually move these into separate files or something and merge when building template
\renewcommand{\d}[1]{\ensuremath{\operatorname{d}\!{#1}}} % dx macro for integrals
\newcommand{\hess}[1]{\ensuremath{\operatorname{H}\!{#1}}} % Hessian
\newcommand{\diff}[1]{\ensuremath{\operatorname{D}\!{#1}}} % Jacobian
\newcommand{\inner}[2]{\left\langle #1, #2 \right\rangle} % inner product
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % norm
\newcommand{\cpl}[1]{\overline{#1}} % complement
\renewcommand{\v}[1]{\mathbf{#1}} % vector
\newenvironment{amatrix}[1]{% augumented matrix - make sure to have # columns less than required amount
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
%-----------------------------------------------------------------------------------------------------------------
% Define theorem environments, along with a custom proof environment
\usepackage[thref, thmmarks,amsmath]{ntheorem}
\newcommand{\itref}[1]{\textit{\thref{#1}}}

\newtheorem{theorem}{Thm.}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Def'n.}
\newtheorem{corollary}[theorem]{Cor.}
\newtheorem{proposition}[theorem]{Prop.}

\theorembodyfont{\upshape}
\newtheorem{remark}[theorem]{Rmk.}
\newtheorem{exercise}[theorem]{Exc.}
\newtheorem{example}[theorem]{Ex.}
\theoremseparator{}
\theoremindent0.0cm
\theoremstyle{nonumberplain}
\theoremheaderfont{\scshape}
\newtheorem{solution}{Sol'n}
\theoremsymbol{$\square$}
\newtheorem{proof}{Proof}

%-----------------------------------------------------------------------------------------------------------------
% Define Document Variables
\newcommand{\assignmentname}{Course Notes}
\newcommand{\classname}{Introduction to Probability}
\newcommand{\semester}{BSM Fall 2018}

% Define a title page for the document
%----------------------------------------------------------------------------------------------------------------------
% Define headings for each page
\usepackage{fancyheadings}
\pagestyle{fancy}
\lhead{Alex Rutar\\arutar@uwaterloo.ca}
\rhead{\classname: \assignmentname\\\semester}
\cfoot{\thepage}
\setlength{\headheight}{50pt}
%----------------------------------------------------------------------------------------------------------------------
\begin{document}
\pagenumbering{roman}
\begin{titlepage}
    \centering
    \vspace{5cm}
    {\huge\textbf{\assignmentname}\par} % Assignment Name
    \vspace{2cm}
    {\Large\textbf{\classname}\par} % Class
    \vspace{3cm}
    {\Large\textit{Alex Rutar}\par}

    \vfill

% Bottom of the page
    {\large \semester \par} % Due Date
\end{titlepage}
%----------------------------------------------------------------------------------------------------------------------
% \newpage\null\thispagestyle{empty}\textit{This page is left intentionally blank.}\newpage
\pagenumbering{roman}
\tableofcontents
\pagenumbering{arabic}
\chapter{Fundamentals}
\section{Basic Principles}
\subsection{Probability Spaces}
A probability space is a triple $(\Omega,\mathcal{F},\mathbb{P})$.
\subsection{$\Omega$}
$\Omega$ is a set, called the sample space, and $\omega\in\Omega$ are called outcomes and $A\subset\Omega$ are called events.
\begin{example}
    A horserace with $3$ horses, $a$, $b$, $c$, has $\Omega=\{(a,b,c),(a,c,b),\ldots,(c,b,a)\}$.
    Then $|\Omega|=6$ and $A=\{a\text{ wins the race}\}=\{(a,b,c),(a,c,b)\}$.
\end{example}
\begin{example}
    Roll two fair dice, a white die and a yellow die.
    Then $\Omega=\{(1,1),(1,2),\ldots,(6,6)\}$ and $|\Omega|=36$.
\end{example}
\begin{example}
    Continue flipping a coin until there is a head.
    Then
    \[\Omega=\{(H),(T,H),(T,T,H),\ldots\}\]
    Then define
    \[A=\{\text{there are an even number of rolls}\}=\{(T,H),(T,T,T,H),\ldots\}\]
\end{example}
\begin{example}
    Consider $\Omega=\{(x,y)\in\R^2\mid x^2+y^2\leq 100\}$.
    Then $A=\{\text{you score 50 points}\}=\{(x,y)\mid x^2+y^2\leq 1\}$.
\end{example}
\begin{definition}
    If $A\cap B=\emptyset$, we say that $A$ and $B$ are \textbf{mutually exclusive} events.
    If $A\subset B$, we say that \textbf{$A$ implies $B$}.
\end{definition}
Write $A^c=\Omega\setminus A$.
Recall distributivity, the deMorgan relations, etc.
\subsection{$\mathcal{F}$}
$\mathcal{F}$ is a collection of subsets of $\Omega$, which denote the events that we consider.
\begin{itemize}[nolistsep]
    \item If $\Omega$ is countable, then typically $\mathcal{F}$ is just the collection of all subsets of $\Omega$.
    \item If $\Omega$ is a domain in $\R^n$, then it is a strict subset of $\R^n$.
\end{itemize}
In any case, $\mathcal{F}$ has to be closed under the following operations:
\begin{enumerate}
    \item $\Omega\in\mathcal{F}$
    \item If $A\in\mathcal{F}$, then $A^c\in\mathcal{F}$
    \item If $A_1,A_2,\ldots\in\mathcal{F}$, then $\bigcup\limits_{i=1}^\infty A_i\in\mathcal{F}$.
\end{enumerate}
in other words, that $\mathcal{F}$ is a $\sigma-$algebra.
\subsection{$\mathbb{P}$}
Finally, $\mathbb{P}:\mathcal{F}\to\R$ is a function that satisfies $3$ axioms:
\begin{enumerate}
    \item For any $A\in\mathcal{F}$, then $\mathbb{P}(A)\geq 0$
    \item $\mathbb{P}(\Omega)=1$
    \item ($\sigma-$additivity) Let $A_1,A_2,A_3,\ldots$ be a sequence of mutually exclusive events.
        Then
        \[\mathbb{P}\left(\bigcup\limits_{i=1}^\infty A_i\right)=\sum\limits_{i=1}^\infty\mathbb{P}(A_i)\]
\end{enumerate}
\subsection{Consequences}
\begin{itemize}
    \item $\mathbb{P}(A^c)+\mathbb{P}(A)=\mathbb{P}(A\cup A^c)=\mathbb{P}(\Omega)=1$.
    \item If $A\subset B$, then $\mathbb{P}(A)\leq\mathbb{P}(B)$ since $\mathbb{P}(B)=\mathbb{P}( (A^c\cap B)\cup (A\cap B) )=\mathbb{P}(A^c\cap B)+\mathbb{P}(A\cap B)=\mathbb{P}(A^c\cap B)+\mathbb{P}(A)$
    \item For any $A,B$, we have
        \[\mathbb{P}(A\cup B)=\mathbb{P}( (A^c\cap B)\cup (A\cap B) \cup (A\cap B^c) )=\mathbb{P}(A^c\cap B)+\mathbb{P}(A\cap B)+\mathbb{P}(B^c\cap A)=\mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A\cap B)\]
        Similarly,
        \[\mathbb{P}(A\cup B\cup C)=\mathbb{P}(A)+\mathbb{P}(B)+\mathbb{P}(C)-\mathbb{P}(A\cap B)-\mathbb{P}(A\cap C)-\mathbb{P}(B\cap C)+\mathbb{P}(A\cap B\cap C)\]
        which generlizes arbitrarily:
        \[\mathbb{P}\left(\bigcup\limits_{i=1}^n A_i\right)=\sum\limits_{r=1}^n(-1)^{r+1}\sum\limits_{1\leq i_1<i_2<\cdots<i_r\leq n}\mathbb{P}(A_{i_1}\cap\cdots\cap A_{i_r})\]
        \begin{proof}
            We have already proved the base case for $n=2$, so assume the formula holds for a union of $n$ events.
            Then
            \begin{align*}
                \mathbb{P}(A_1\cup\cdots A_n\cup A_{n+1}) &= \mathbb{P}(A_1\cup\cdots\cup A_n)+\mathbb{P}(A_{n+1})-\mathbb{P}( (A_1\cup\cdots\cup A_n)\cap A_{n+1} )
            \end{align*}
            We can distribute the first and third terms using the induction hypothesis, and the result follows.
        \end{proof}
\end{itemize}
\begin{definition}
    We say $D_1,D_2,\ldots$ is a \textbf{decreasing} sequence of events of $D_{k+1}\subset D_k$.
    We say $D_1,D_2,\ldots$ is a \textbf{increasing} sequence of events of $D_{k+1}\supset D_k$.
\end{definition}
Let $\lim_{n\to\infty}D_n=\bigcap_{n=1}^\infty D_n$ and $\lim_{n\to\infty}I_n=\bigcup_{n=1}^\infty I_n$.
\begin{proposition}
    $\sigma-$additivity implies that for any increasing sequence,
    \[\Pr\left(\lim_{n\to\infty}I_n\right)=\lim_{n\to\infty}\Pr(I_n)\]
    and similarly for any decreasing sequence
    \[\Pr\left(\lim_{n\to\infty}D_n\right)=\lim_{n\to\infty}\Pr(D_n)\]
\end{proposition}
\begin{proof}
    Note that (2) implies (1): if $D_k$ is a decreasing sequence, then $I_k=D_k^c$ is an increasing sequence and
    \[\left(\lim_{n\to\infty}D_n\right)^c=\left(\bigcap_{n=1}^\infty D_n\right)^c=\bigcup_{n=1}^\infty I_n=\lim_{n\to\infty}I_n\]
    and taking probabilities,
    \[\Pr\left(\lim_{n\to\infty}D_n\right)=1-\Pr\left(\lim_{n\to\infty}I_n\right)=1-\lim_{n\to\infty}\Pr(I_n)=\lim_{n\to\infty}\Pr(D_n)\]
    To prove that $\sigma-$additivity implies (1), let $I_1,I_2,\ldots$ be increasing.
    Let $A_1=I_1$ and for $k\geq 2$ let $A_k=I_k\setminus I_{k-1}$.
    Then $A_1,A_2,\ldots$ are mutually exclusive and for any $k\geq 1$, 
    \[\bigcup_{k=1}^K A_k=I_k\]
    Thus
    \[\bigcup\limits_{k=1}^\infty A_k=\lim_{n\to\infty}I_n\]
    Now note that $\Pr(I_K)=\sum_{k=1}^K\Pr(A_k)$ while
    \begin{align*}
        \Pr\left(\lim_{n\to\infty}I_n\right) &= \Pr\left(\bigcup\limits_{k=1}^\infty A_k\right)\\
                                             &= \sum\limits_{k=1}^\infty \Pr(A_k)\\
                                             &= \lim_{K\to\infty}\sum\limits_{k=1}^K\Pr(A_k)\\
                                             &= \lim_{K\to\infty}\Pr(I_K)
    \end{align*}
\end{proof}
\subsection{Examples with Finite Uniform Probabilities}
We assume that $\Omega=\{\omega_1,\omega_2,\dots,\omega_N\}$ and $\mathbb{P}(\{\omega_i\})=\mathbb{P}(\{\omega_j\})$.
Then $\mathbb{P}(\{\omega_i\})=\frac{1}{N}$ and $\mathbb{P}(A)=|A|/N$.
\begin{example}
    In an urn there are 6 blue balls and 5 red balls.
    Draw 3 balls out of this 11.
    What is the change that among the 3 there are exactly 2 blue balls and 1 red ball?

    Let us pretend that the balls are labelled, 1 through 11, and set $\Omega$ to be all the ordered triples of disjoint elements.
    Then $A=\{\text{exactly 2 blue and 1 red}\}$, and note that $A=A^1\cup A^2\cup A^3$ where $A^i$ has a red in position $i$ and blue in the other two positions.
    Now, $|A^i|=5\cdot 6\cdot5$, so $|A|=3\cdot6\cdot5\cdot 6$ and
    \[\mathbb{P}(A)=\frac{|A|}{|\Omega|}=\frac{3\cdot 6\cdot 5\cdot 6}{11\cdot 10\cdot 9}\]

    We now suppose that $\Omega=\{\Lambda\subset\{1,\ldots,11\}\mid |\Lambda|=3\}$, so $|\Omega|=\binom{11}{3}$.
    Now
    \[A=\{\Lambda_1\cup\Lambda_1|\Lambda_1\subset\{1,\ldots,6\},|\Lambda_1|=2,\Lambda_2\subset\{7,\ldots,11\},|\Lambda_2|=1\}\]
    So $|A|=\binom{6}{2}\cdot 5$.
\end{example}
\begin{example}
    Consider a group of $N$ people.
    What is the chance that there is at least one pair amoung them who have the same birthday?

    Define $\Omega=\{(i_1,i_2,\ldots,i_N)\mid i_j\in\{1,\ldots,365\}\}$.
    We want $A=\{\text{there is at least one common birthday}\}$.
    We can write
    \[A^c=\{(i_1,\ldots,i_n)\in\Omega\mid i_j\neq i_k \forall j\neq k\}\]
    Then $|A^c|=365\cdot 364\cdots (365-N+1)$ and
    \[P_N=\mathbb{P}(A)=1-\mathbb{P}(A^c)=1-\frac{365\cdot364\cdots(365-N+1)}{365^N}\]
\end{example}
\begin{example}
    Suppose we have $N$ people at a party.
    The following day, everyone leaves one after another, and chooses a single phone from a pile.
    What is the chance that nobody chooses her own phone?

    Define $\Omega=\{(i_1,\ldots,i_N)\mid\text{permutations of }\{1,\ldots,N\}\}$, so $\omega=(i_1,\ldots,i_k)$ means person $k$ chooses phone $i_k$.
    Then $|\Omega|=N!$.
    Fix $B=\{\text{nobody picks her/his phone}\}$.
    Define $A_1=\{\text{person 1 picks his phone}\}$, so $|A_1|=(N-1)!$, and similarly for $A_2$, etc.
    Then $B=A_1^c\cap A_2^c\ldots\cap A_N^c=(A_1\cup \ldots\cup A_N)^c$, and $\mathbb{P}(A_i)=\frac{1}{N}$.
    Now in general,
    \[\Pr(A_{i_1}\cap\cdots\cap A_{i_k})=\frac{(N-k)!}{N!}\]
    for $i_k$ distinct.
    Thus we now have
    \begin{align*}
        \Pr(B) &= 1-\Pr(A_1\cup A_2\cup\ldots\cup A_n)\\
               &= 1-\sum\limits_{r=1}^N(-1)^{r+1}\sum\limits_{1\leq i_1<i_2\cdots<i_r\leq N}\Pr(A_{i_1}\cap\cdots\cap A_{i_r})\\
                      &= \sum\limits_{r=1}^n(-1)^{r+1}\binom{N}{r}\frac{(N-r)!}{N!}\\
                      &= \sum\limits_{r=1}^N(-1)^{r+1}\frac{1}{r!}
    \end{align*}
    so that
    \[\Pr(B)=1+\sum\limits_{r=1}^N(-1)^r\frac{1}{r!}=\sum\limits_{r=0}^N(-1)^r\frac{1}{r!}\]
    Thus $\lim_{N\to\infty}\Pr(B)=\frac{1}{e}$.
\end{example}
\begin{example}[Round table seating]
    Consider a round table with 20 seats, and 10 married couples sit.
    What is the change that no couples sit together?

    Define $\Omega=\{\text{permutations of }\{1,\ldots,20\}\}/\sim$ where $(i_1,\ldots,i_{20})\sim(i_{20},i_1,\ldots,i_{19})$.
    Then $|\Omega|=19!$.
    Define $B=\{\text{no couples together}=A_1^c\cap A_2^c\cap\cdots\cap A_{10}^c\}$, where
    \[A_k=\{\text{the 8th woman sits next to her spouse}\}\]
    so that
    \[\Pr(B)=1-\Pr(A_1\cup\cdots\cup A_{10})\]
    Note that
    \[\Pr(A_i)=\frac{18!2}{19!}=\frac{2}{19}\]
    by ``joining'' the couple together, arranging them around the table, and permuting the couple internally.
    Thus generalizes to
    \[\Pr(A_{i_1}\cap\cdots\cap \Pr(a_{i_r})=\frac{2^r(19-r)!}{19!}\]
    Then by inclusion-exclusion,
    \[\Pr(B)=1-\binom{10}{1}\cdot\frac{18!2}{19!}+\binom{10}{2}\frac{17!2^2}{19!}-\binom{10}{3}\frac{16!2^3}{19!}\cdots+\binom{10}{10}\frac{9!2^{10}}{19!}\approx 0.339\]
\end{example}
\begin{example}[Poker hand probabilities]
    A poker hand is a straight if the 5 cards are of increasing value and not all of the same suit, starting with $A,2,3,4,\ldots,10$.

    Define $\Omega=\{\text{5 element subsets of the 52 cards}\}$.
    Then $|\Omega|=\binom{52}{5}$.
    Thus
    \[\Pr(\text{straight})=\frac{10\cdot (4^5-4)}{\binom{52}{5}}\]
    \[\Pr(\text{full house})=\frac{13\cdot 12\cdot\binom{4}{3}\cdot\binom{4}{2}}{\binom{52}{5}}\]
\end{example}
\begin{example}[Bridge hand probabilities]
    In bridge, each of the 4 players get 13 cards.
    Let $\Omega=\{\text{13 cards that North gets}\}$.
    \begin{enumerate}
        \item Let $E$ denote the event that North receives all spades.
            Then
            \[\Pr(E)=\frac{1}{\binom{52}{13}}\]
        \item Now let $E$ denote the event that north does not receive all 4 suits of any value, so $E^c$ is the event that North receives all 4 of some suit.
            Thus $\Pr(E)=1-\Pr(E^c)$.
            Let $V_k=$ denote the event that North gets all suits of suit $k$.
            Then
            \begin{align*}
                \Pr(V_1) &= \frac{\binom{48}{9}}{\binom{52}{13}}\\
                \Pr(V_1\cap V_2) &= \frac{\binom{44}{5}}{\binom{52}{13}}\\
                \Pr(V_1\cap V_2\cap V_3) &= \frac{\binom{40}{1}}{\binom{52}{13}}
            \end{align*}
            So that, by Inclusion Exclusion,
            \[1-\Pr(V_1\cup V_2\cup\cdots\cup V_{13})=1-\frac{\binom{48}{9}}{\binom{52}{13}}\cdot 13+\binom{13}{2}\frac{\binom{44}{5}}{\binom{52}{13}}-\binom{13}{3}\frac{40}{\binom{52}{5}}\]
    \end{enumerate}
    What is the change that each player receives one ace?
    There are
    \[\frac{52!}{13!13!13!13!}\]
    possible hands.
    There are $4!$ ways to arrange the aces, which gives
    \[\Pr(E)=\frac{4!\binom{48}{12,12,12,12}}{\binom{52}{13,13,13,13}}\]
\end{example}
\section{Conditional Probability}
\subsection{Basic Principles}
Suppose we roll two fair dice.
Then $\Pr(\text{the sum is 10})=\frac{3}{36}=\frac{1}{12}$.
Suppose instead that the white dice is rolled first, and it turns up $6$.
Now the probability that the sum is 10 is now $1/6$.
\begin{definition}
    Given an even $E$ with $\Pr(E)>0$, for any event $F$, let $\Pr(F|E)=\frac{\Pr(F\cap E)}{\Pr(E)}$.
    We call this the \textbf{conditional probability of $F$ given $E$}.
\end{definition}
\begin{proposition}
    Fix $E$ with $\Pr(E)>0$ and consider $\Pr(\cdot|E):\mathcal{F}\to\R$.
    This function satisfies the axioms of probability.
\end{proposition}
\begin{proof}
    \begin{enumerate}
        \item $\Pr(F|E)\geq0$ for all $F\in\mathcal{F}$.
        \item $\Pr(\Omega|E)=\frac{\Pr(E\cap\Omega)}{\Pr(E)}=1$
        \item If $F_1,F_2,\ldots$ are mutually exclusive, then
            \begin{align*}
                \Pr\left(\bigcup\limits_{i=1}^\infty F_i|E\right) &= \frac{\Pr( (\bigcup_{i=1}^\infty F_i)\cap E)}{\Pr(E)}\\
                                                      &= \frac{\Pr(\bigcup_{i=1}^\infty(E\cap F_i))}{\Pr(E)}\\
                                                      &= \sum\limits_{n=1}^\infty\frac{\Pr(F_i\cap E)}{\Pr(e)}\\
                                                      &= \sum\limits_{n=1}^\infty\Pr(F_n|E)
            \end{align*}
    \end{enumerate}
\end{proof}
\begin{proposition}
    We have $\Pr(E\cap F)=\Pr(F|E)\cdot \Pr(E)$, and more generally
    \[\Pr(E_n\cap E_{n-1}\cap\cdots\cap E_1)=\Pr(E_n|E_{n-1}\cap\cdots\cap E_1)\cdots\Pr(E_3|E_2\cap E_1)\Pr(E_2|E_1)\Pr(E_1)\]
\end{proposition}
\begin{proof}
    This follows by induction from the definition of conditional probability.
\end{proof}
\begin{example}
    Andrew and Bob play for the college basketball team.
    They get two T-shirts each, in closed bags.
    Any T-shirt can be black or white, with 50-50 chance.
    Andrew prefers black, but Bob has no preference.
    The following day, Andrew shows up with a black shirt on.
    What is the chance that Andrew's other shirt is black?
\end{example}
\begin{solution}
    We have $\Omega=\{(B,B),(B,W),(W,B),(W,W)\}$ which is reduced to $\{(B,B),(B,W),(W,B)\}$, so the answer is $1/3$.
    To make this transparent, consider
    \begin{align*}
        A_1&=\{\text{Andrew has at least one black shirt}\}\\
        A_2&=\{\text{Both of Andrew's shirts are black}\}\\
        A_3&=\{\text{Andrew has a black shirt on}\}
    \end{align*}
    so in Andrew's case, $A_1=A_3$ and $\Pr(A_2|A_3)=\Pr(A_2|A_1)$.
\end{solution}
\begin{example}[Polya's Urn]
    Initially, we have two balls, 1 red, 1 blue, in the urn.
    For the first draw, pick one, check its color, and put it back and put another ball of the same color into the urn.
    What is the probability that the first three balls are red, blue, red (in this order)?
\end{example}
\begin{solution}
    Let $R_i,B_i$ denote the $i^{th}$ draw is red or blue respectively.
    Then
    \[\Pr(R_3\cap B_2\cap R_1)=\Pr(R_3|B_2\cap R_1)\Pr(B_2|R_1)\Pr(R_1)=\frac{1}{2}\frac{1}{3}\frac{1}{2}=\frac{1}{12}\]
\end{solution}
\begin{example}
    What is the probability in bridge, each of the players gets one ace?
\end{example}
\begin{solution}
    Let $E_4$ denote the event in which every player gets an ace.
    Then
    \begin{align*}
        E_4&\\
        \cap&\\
        E_3&=\{\text{Aces of spaces, heards, and diamonds are at 3 different players.}\}\\
        \cap&\\
        E_2&=\{\text{Aces of spaces, hearts, and diamonds are at 2 diferent players.}\}\\
        \cap&\\
        E_1&=\Omega
    \end{align*}
    so that $\Pr(E_4)=\Pr(E_4\cap E_3\cap E_2\cap E_1)=\Pr(E_4|E_3)\Pr(E_3|E_2)\Pr(E_2|E_1)\Pr(E_1)$.
\end{solution}
\subsection{Bayes' Formula}
\begin{example}
    Consider an insurance compacy, which classifies people into accident prone drivers (30\%) and non-accident-prone drivers, (70\%).
    For accident prone drivers, the chance of being involved in an accident within a year is 0.2, while for non-addicent-prone drivers, the chance of being involved in an accident is 0.1.
    Now suppose we have a new policyholder.
    \begin{enumerate}[nolistsep]
        \item What is the probability that the policyholder is involved in an accident within a year?
        \item The policyholder was involved in an accident?
    \end{enumerate}
\end{example}
\begin{solution}
    \begin{enumerate}
        \item $B=\{\text{accident in 2018}\}$, $A=\{\text{the policyholder is accident prone}\}$.
            Then
            \[\Pr(B)=\Pr(B\cap A)+\Pr(B\cap A^c)=\Pr(B|A)\Pr(A)+\Pr(B|A^c)\Pr(A^c)=0.2\cdot 0.3+0.1\cdot 0.7=0.13\]
        \item Now
            \[\Pr(A|B)=\frac{\Pr(A\cap B)}{\Pr(B)}=\frac{\Pr(B|A)\Pr(A)}{\Pr(B|A)\Pr(A)+\Pr(B|A^c)\cdot\Pr(A^c)}=\frac{0.2\cdot 0.3}{0.13}=\frac{6}{13}\]
    \end{enumerate}
\end{solution}
\begin{proposition}
    Suppose $A_1,A_2,\ldots,A_n\in\mathcal{F}$ form a partition of $\Omega$.
    Given such a partition, for any $B\in\mathcal{F}$,
    \[\Pr(B)=\sum\limits_{i=1}^n\Pr(B\cap A_i)=\sum\limits_{i=1}^n\Pr(B|A_i)\cdot\Pr(A_i)\]
    Then for any $k\in[n]$,
    \[\Pr(A_k|B)=\frac{\Pr(B\cap A_k)}{\Pr(B)}=\frac{\Pr(B|A_k)\cdot\Pr(A_k)}{\sum\limits_{i=1}^n\Pr(B|A_i)\cdot\Pr(A_i)}\]
\end{proposition}
\begin{example}
    Roll a fair dice.
    There is a urn with one white ball in it.
    If the die turns up 1,3, or 5, put one black ball ito the urn.
    If it turns up 2 or 4, put 3 black and 5 white, and if it turns up 6, put 5 black and 5 white.
\end{example}
\begin{solution}
    Write
    \begin{align*}
        A_1 &= \{\text{1,3 or 5 rolled}\}\\
        A_2 &= \{\text{2 or 4 rolled}\}\\
        A_3 &= \{\text{6 rolled}\}\\
        B &= \{\text{black ball rolled}\}
    \end{align*}
    so that
    \begin{align*}
        \Pr(A_3|B) &= \frac{\Pr(B|A_3)\Pr(A_3)}{\Pr(B|A_1)\cdot \Pr(A_1)+\Pr(B|A_2)\cdot\Pr(A_2)+\Pr(B|A_3)\cdot\Pr(A_3)}\\
                   &= \frac{5/6\cdot 1/6}{1/2\cdot 1/2+3/4\cdot1/3+5/6\cdot 1/6}\\
                   &=\frac{5}{23}
    \end{align*}
\end{solution}
\begin{example}
    There is a blood test for a rare but serious disease.
    Only 1/10000 people have this disease.
    Suppose the test is 100\% effective, so if someone is tested ill, it is positive with 100\% chance.
    Suppose there is also a 1\% chance of false positive.
    A new patient is tested, and tests positive.
    What are the odds that she has the disease?
\end{example}
\begin{solution}
    Let $A=\{\text{the person is ill}\}$ and $B=\{\text{the test is positive}\}$.
    Then
    \[\Pr(A|B)=\frac{\Pr(B|A)\Pr(A)}{\Pr(B|A)\Pr(A)+\Pr(B|A^c)\Pr(A^c)}=\frac{1\cdot 0.0001}{1\cdot 0.0001+0.01\cdot0.9999}\]
\end{solution}
\begin{example}[Monty Hall paradox]
    There are three doors: one of them hides a prize, and two hide nothing.
    Pick a door.
    The announcer then reveals another door not containing a prize.
    Is it better to stay or switch?
\end{example}
\begin{solution}
    Write $A_i=\{\text{door $i$ hides the price}\}$, and $B_2=\{\text{door $2$ is opened}\}$.
    Then
    \begin{align*}
        \Pr(A_1|B_2)&=\frac{\Pr(B_2|A_1)\Pr(A_1)}{\Pr(B_2|A_1)\Pr(A_1)+\Pr(B_2|A_2)\Pr(A_2)+\Pr(B_2|A_3)\Pr(A_3)}\\
                                         &=\frac{1/2\cdot1/3}{1/2\cdot1/3+0+1\cdot1/3}=\frac{1}{3}
    \end{align*}
    but
    \begin{align*}
        \Pr(A_3|B_2)&=\frac{\Pr(B_2|A_3)\Pr(A_3)}{\Pr(B_2|A_1)\Pr(A_1)+\Pr(B_2|A_2)\Pr(A_2)+\Pr(B_2|A_3)\Pr(A_3)}\\
                                         &=\frac{1\cdot1/3}{1/2\cdot1/3+0+1\cdot1/3}=\frac{2}{3}
    \end{align*}
    so it is better to switch!
\end{solution}
\begin{example}
    There is an inspection, which is 60\% sure of the guilt of a certain suspect.
    The suspect is left-handed.
    There is new evidence: the criminal is left handed.
    Say 20\% of the population is left handed; how certain should the inspector now be?
\end{example}
\begin{solution}
    Write $C=\{\text{the suspect is the criminal}\}$ and $C^c=\{\text{the criminal is someone else}\}$.
    Then $\Pr(C)=0.6$ and $\Pr(C^c)=0.4$.
    Let $L=\{\text{the criminal is left-handed}\}$.
    Then
    \[\Pr(C|L)=\frac{\Pr(L|C)\Pr(C)}{\Pr(L)}\qquad\Pr(C^c|L)=\frac{\Pr(L|C^c)\Pr(C^c)}{\Pr(L)}\]
    Here, we can compute the ``odds'':
    \[\frac{\Pr(C|L)}{\Pr(C^c|L)}=\frac{\Pr(L|C)\Pr(C)}{\Pr(L|C^c)\Pr(C^c)}\]
    Now $\Pr(L|C)=1$, but $\Pr(L|C^c)=\Pr(L)=0.2$, since the probability is taken a priori.
    Now a priori, the odds are given by $\Pr(C)/\Pr(C^c)=0.6/0.4$, scaled by the factor $\Pr(L|C)/\Pr(L|C^c)=5$ given updated information.
    Thus $\Pr(C|L)=15/17$.
\end{solution}
\section{Independent Events}
\subsection{Definitions}
\begin{definition}
    The events $A$ and $B$ are \textbf{independent} if $\Pr(A\cap B)=\Pr(A)\Pr(B)$.
\end{definition}
\begin{example}
    Draw a card from a deck of 52.
    Let
    \[A=\{\text{it is a spade}\},\quad B=\{\text{it is an ace}\},\quad C=\{\text{it is a heart}\}\]
    We have
    \[\Pr(A)=\frac{1}{4},\quad\Pr(B)=\frac{1}{13},\quad\Pr(A\cap B)=\frac{1}{52}\]
    so $A$ and $B$ are independent.
    Similarly, $B$ and $C$ are independent.
    However, $\Pr(A\cap C)=0\neq 1/4$ so $A$ and $C$ are not independent.
\end{example}
\begin{remark}
    Exclusive events are quite different than independence: in fact, they are (in a sense) the opposite.
    Let $\Pr(A)>0$.
    Then $A$ and $B$ are independent iff $\Pr(B|A)=\Pr(B)$.
    Similarly, $A$ and $B$ are exclusive iff $\Pr(B|A)=0$.
\end{remark}
\begin{example}
    Roll two fair dice, the yellow and the white die.
    Then
    \begin{align*}
        A &= \{\text{the sum is 7}\}\\
        B &= \{\text{the sum is 10}\}\\
        C &= \{\text{the yellow die turns up 6}\}\\
        D &= \{\text{the white die turns up 6}\}
    \end{align*}
    We have $\Pr(A)=1/6$, $\Pr(C)=1/6$.
    Then $\Pr(A\cap C)=1/36=1/6\cdot 1/6$ so $A$ and $C$ are independent.
    Similarly, $C$ and $D$ are independent and $A$ and $D$ are independent.
    Thus $A$, $C$, $D$ are pairwise independent but not independent as a triple.
\end{example}
\begin{definition}
    The events $A_1,A_2,\ldots$ are \textbf{independent (as a collection)} if, for any choice of indices $1\leq i_1<i_2<\cdots<i_k\leq n$, then
    \[\Pr(A_{i_1}\cap A_{i_2}\cap\cdots\cap A_{i_k})=\Pr(A_{i_1})\Pr(A_{i_2})\cdots\Pr(A_{i_k})\]
\end{definition}
\subsection{Independent Trials}
We have two parameters: $n\geq 1$, which is the number of trials, and $p\in(0,1)$, which is the chance of success for an individual trial.
Then $A_k=\{\text{the $k^\text{th}$ trial is a succes}\}$ so that $\Pr(A_k)=p$ and the events $A_1,\ldots,A_n$ are independent.
Our framework is to consider the space $\Omega\times\Omega\times\cdots\times\Omega$.
\begin{example}
    Roll a fair die 10 times.
    Then $A_k=\{\text{the $k^\text{th}$ roll is a 6}\}$.
    Then we have
    \begin{itemize}
        \item $\Pr(\text{all $n$ trials are successful})=\Pr(A_1\cap \cdots\cap A_n)=p^n$
        \item $\Pr(\text{there is at least one success})=1-(1-p)^n$
        \item $\Pr(\text{there are exactly $k$ success out of $n$ trials})=\binom{n}{k}p^k(1-p)^{n-k}$
    \end{itemize}
\end{example}
Consider the case now where $n$ is countable (infinite number of trials).
Let $S=\{\text{all trials are successful}\}$ define and $S_n=\{\text{the first $n$ trials are successful}\}$.
Then $S=\bigcap\limits_{n=1}^\infty S_n$ so
\[\Pr(S)=\lim_{n\to\infty}\Pr(S_n)=\lim_{n\to\infty}p^n=0\]
\begin{example}
    Repeatedly roll two fair dice until the sum is either 5 or 7.
    What is the probability that the sum is $5$ when we stop?

    Let $A_i=\{\text{rolls less than $i$ are not 5 or 7, roll $i$ is 5}\}$.
    Since $\Pr(\text{roll is 5 or 7})=1/6+1/9$, we have $\Pr(\text{roll is not})=13/18$.
    Thus
    \[\Pr(A_i)=\left(\frac{13}{18}\right)^{i-1}\frac{5}{18}\]
    so that
    \[\Pr(A)=\frac{1}{9}\sum\limits_{i=0}^\infty\left(\frac{13}{18}\right)^i=\frac{1}{9}\frac{1}{1-\frac{13}{18}}=\frac{2}{5}\]
\end{example}
We have an alternate solution: note that $A_1,B_1,C_1$ partition the sample space.
By the law of total probability,
\begin{align*}
    \Pr(D)&=\Pr(D|A_1)\Pr(A_1)+\Pr(D|A_2)\Pr(A_2)+\Pr(D|C_1)\Pr(C_1)\\
          &= \Pr(B_1)+\Pr(C_1)\Pr(D)
\end{align*}
so that
\[\Pr(D)=\frac{\Pr(B_1)}{1-\Pr(C_1)}=\frac{\Pr(B_1)}{\Pr(A_1)+\Pr(B_1)}\]
\subsection{Random Walks}
We first see the gambling interpretation.
Suppose we have two players, $A$ has initial capital $k$ and $B$ has initial capital $N-k$.
At each round, a coin is flipped.
If it is a head, then $B$ gives $A$ 1 dollar, and if it is a tail, $A$ gives $B$ 1 dollar.
Repeat this until someone runs out of money.
\begin{center}
    \begin{tikzpicture}[nd/.style={inner sep=2pt,circle,fill=black}]
        \node[nd,label=below:{0}] (0) at (0,0){};   
        \node[nd,label=below:{1}] (1) at (1,0){};
        \node[nd,label=below:{2}] (2) at (2,0){};
        \node[nd,label=below:{3}] (3) at (3,0){};
        \node[nd] (kb) at (4.5,0){};
        \node[nd,label=below:{$k$}] (k) at (5,0){};
        \node[nd] (ka) at (5.5,0){};
        \node[nd,label=below:{$N-1$}] (Nm1) at (7,0){};
        \node[nd,label=below:{$N$}] (N) at (8,0){};
        \draw (0)--(1)--(2)--(3)--(k)--(Nm1)--(N);
        \draw plot [smooth, tension=1.4] coordinates { (kb) (4.75,0.25) (k)};
        \draw plot [smooth, tension=1.4] coordinates { (ka) (5.25,0.25) (k)};
    \end{tikzpicture}
\end{center}
Let $\Pr_k^{(N)}=\Pr(\text{when starting at position $j$, the probability that eventually $A$ wins})$.
We have $P_0=0$, $P_N=1$.
Then for $1\leq k\leq N-1$, we have
\[P_k=\Pr\{\text{ending at $N$ when starting at $k$}|\text{first flip is H}\}\cdot\frac{1}{2}+\Pr\{\text{end at $N$ if start at $k$}|\text{first flip is T}\}\cdot\frac{1}{2}\]
which can be written
\[\P_k=P_{k+1}\frac{1}{2}+P_{k-1}\frac{1}{2}\Rightarrow \frac{1}{2}\left(P_k-P_{k-1}\right)=\frac{1}{2}\left(P_{k+1}-P_k\right)\]
so, for any $1\leq k\leq N$, $P_k-P_{k-1}=d$ and
\[1=P_N-P_0=P_n-P_{N-1}+P_{N-1}-P_{N-2}+\cdots+(P_1-P_0)=N\cdot d\]
so $d=1/N$ and
\[P_k=P_k-P_0=\sum\limits_{j=1}^k(P_j-P_{j-1})=kd=\frac{k}{N}\]
\subsection{Conditional Independence}
\begin{definition}
    Given $A$ with $\Pr(A)>0$, two events $B_1$ and $B_2$ are \textbf{conditionally independent} given $A$ if
    \[\Pr(B_1\cap B_2|A)=\Pr(B_1|A)\cdot\Pr(B_2|A)\]
\end{definition}

\begin{example}
    \begin{enumerate}
        \item We have a medical test for a rare disease, and $A=\{\text{the patient is sick}\}$ has $\Pr(A)=0.005$ so $\Pr(A^c)=0.995$.
            Let $B_1=\{\text{the first test is positive}\}$, so $\Pr(B_1|A)=0.95$ and $\Pr(B_1|A^c)=0.01$.
            Then $\Pr(A|B)\approx 0.33$.
            But now let $B_2=\{\text{the second test is positive}\}$.
            Now what is $\Pr(A|B_1\cap B_2)$?
            Here, the events $B_1$ and $B_2$ are not independent, but they are conditionally independent given either $A$ or $A^c$.
            Thus
            \begin{align*}
                \Pr(A|B_1\cap B_2)&=\frac{\Pr(B_1\cap B_2|A)\Pr(A)}{\Pr(B_1\cap B_2)}\\
                                  &= \frac{\Pr(B_1|A)\Pr(B_2|A)\Pr(A)}{\Pr(B_1|A)\Pr(B_2|A)\Pr(A)+\Pr(B_1|A^c)\Pr(B_2|A^c)\Pr(A^c)}\\
                                  &= \frac{(0.95)^2\cdot 0.005}{(0.95)^2\cdot 0.005+(0.01)^2\cdot 0.995}\\
                                  &\approx 0.98
            \end{align*}
        \item Suppose
            \begin{align*}
                A &= \{\text{accident prone}\} & \Pr(A) &= 0.3\\
                A &= \{\text{not accident prone}\} & \Pr(A^c) &= 0.7
            \end{align*}
            and let $B_{Y}=\{\text{accident in year $Y$}\}$.
            We have seen that $\Pr(B_{2018}|A)=0.2$ and $\Pr(B_{2018}|A^c)=0.1$ so $\Pr(B_{2018})=0.13$.
            Now
            \begin{align*}
                \Pr(B_{2019}|B_{2018}) &= \frac{\Pr(B_{2018}\cap B_{2019})}{\Pr(B_{2018})}\\
                                       &= \frac{\Pr(B_{2019}|A)\Pr(B_{2018}|A)\Pr(A)+\Pr(B_{2019}|A)\Pr(B_{2018}|A^c)\Pr(A^c)}{\Pr(B_{2018}|A)\Pr(A)+\Pr(B_{2018}|A^c)\Pr(A^c)}\\
                                       &= \Pr(B_{2019}|A)\cdot\Pr(A|B_{2018})+\Pr(B_{2019}|A^c)\Pr(A^c|B_{2018})\\
                                       &= 0.2\cdot\frac{6}{13}+0.1\cdot\frac{7}{13}\\
                                       &\approx 0.15
            \end{align*}
    \end{enumerate}
\end{example}
\begin{example}[Laplace's Rule of Succession]
    Suppose we have $k+1$ coins in a box, and coin $i$ turns up Heads with $\frac{i}{k}$ chance, and Tails with $\frac{k-i}{k}$ chance (for $i=0,\ldots,k$).
    Pick one coin, and flip the coin $n$ times.
    Assume it turned Heads every $n$ times.
    What is the probability that it turns up $H$ on the $(n+1)^\text{st}$ flip?
\end{example}
\begin{solution}
    Let $H_j$ denote the event that the $j^\text{th}$ flip is H for $j=1,2,\ldots,n,n+1$.
    Let $C_i$ denote the event in which the $i^\text{th}$ coin is initially picked for each $i=1,\ldots,k$.
    The events $H_j$ are not independent, but they are conditionally independent given any of the $C_i$.
    Moreover, $\Pr(H_j|C_k)=\frac{i}{k}$.
    We thus have
    \begin{align*}
        \Pr(H_{n+1}|H_1\cap H_2\cap\cdots\cap H_n) &= \frac{\Pr(H_1\cap H_2\cap\cdots\cap H_{n+1})}{\Pr(H_1\cap\cdots\cap H_n)}\\
                                                   &= \frac{\sum\limits_{i=0}^k\Pr\left(\bigcap_{j=1}^{n+1}H_j|C_i\right)\Pr(C_i)}{\sum\limits_{i=0}^k\Pr\left(\bigcap\limits_{j=1}^nH_j|C_k\right)\Pr(C_k)}\\
                                                   &= \frac{\sum\limits_{i=0}^k\prod\limits_{j=1}^{n+1}\Pr(H_j|C_i)\Pr(C_i)}{\sum\limits_{i=0}^k\prod\limits_{j=1}^n\Pr(H_j|C_i)\Pr(C_i)}\\
                                                   &= \frac{\sum\limits_{i=0}^k\left(\frac{i}{k}\right)^{n+1}\frac{1}{k+1}}{\sum\limits_{i=0}^k\left(\frac{i}{k}\right)^n\frac{1}{k+1}}\\
                                                   &:= p(k,n)
    \end{align*}
    Both the numerator and denominator of $p(k,n)$ are sums of the form $\sum\limits_{i=0}^k f(i/k)\cdot 1/k$.
    These are riemann sums of the function $f$ on the interval $[0,1]$, so as $k$ goes to infinity,
    \[\lim_{k\to\infty}p(k,n)=\frac{\int_0^1 x^{n+1}\d{x}}{\int_0^1 x^n\d{x}}=\frac{\frac{1}{n+2}}{\frac{1}{n+1}}=\frac{n+1}{n+2}\]
\end{solution}
\begin{example}[Best prize problem]
    Suppose we have $N$ items, each with a distinct real value.
    Observe them sequentially.
    After observing a prize, you can take the prize, or can abandon it (and never access it again).
    How can you maximize the odds that you get the best prize?
\end{example}
\begin{solution}
    We will solve this over a fixed type of strategy.
    Define a $k-$strategy for each $k=1,\ldots,N$, in which we observe the first $k$ items, and pick the first of the remaining ones that is better than the first $k$.
    Let $B_k$ denote the event that we choose the best item with a $k-$strategy, and fix $P_k^{(N)}=\Pr(B_k)$.
    As well, let $A_i$ denote the event that the best prize is at the $i^\text{th}$ position, so $\Pr(A_i)=1/N$.

    Now, $\Pr(B_k|A_j)=0$ for $j\leq k$, and $\Pr(B_k|A_j)=\frac{k}{j+1}$ for $j>k$.
    Then
    \begin{align*}
        \Pr(B_k) &= \sum\limits_{i=1}^n\Pr(B_k|A_i)\Pr(A_i)\\
                 &= \sum\limits_{i=k}^{N-1}\frac{k}{i}\cdot\frac{1}{N}\\
                 &:= P_k^{(N)}
    \end{align*}
    To determine the optimal strategy, we want to choose the proportion $x$ of items to observe.
    To be precise, let $N\to\infty$ and $k$ be such that $k/N\to x$.
    Then
    \begin{align*}
        \lim_{k/N\to x}P_k^{(N)} &= \lim_{k/N\to x}\sum\limits_{i=k}^{N-1}\frac{k/N}{i/N}\cdot\frac{1}{N}\\
            &= \lim_{k/N\to x}x\sum\limits_{i=k}^{N-1}\frac{1}{i/N}\frac{1}{N}\\
            &= x\int_x^1\frac{1}{y}\d{y}\\
            &= -x\ln x := g(x)
    \end{align*}
    Then $g'(x)=-\ln x-1$ and $g''(x)=-\frac{1}{x}$.
    Then $g'(x)=0\Rightarrow \ln x=-1$ so $x=1/e$ is a maximum since $g''(1/e)<0$.
    \begin{center}
        \begin{tikzpicture}[scale=3]
            \draw[->] (0,0) -- (1.2,0) node[right] {$x$};
            \draw[->] (0,0) -- (0,0.6) node[above] {$y$};
            \draw[scale=1,domain=0.01:1,smooth,variable=\x,blue] plot ({\x},{-\x*ln(\x)});
        \end{tikzpicture}
    \end{center}
\end{solution}
\chapter{Discrete Random Variables}
\section{Basics}
\begin{definition}
    A \textbf{random variable} is a (measurable) function $X:\Omega\to\R$.
\end{definition}
For example, fix $a<b\in\R$ and consider the set $\{w\in\Omega\mid\mathbb{X}(w)\in[a,b]\}\in\mathcal{F}$.
\begin{example}
    \begin{enumerate}
        \item Flip three fair coins.
            Let $Y$ denote the number of Heads.
            Then $Y:\Omega\to\{0,1,2,3\}$.
        \item Repeatedly roll a fair die until a 6 occurs.
            Let $Z$ denote the number of rolls necessary.
            Now $Z:\Omega\to\N$.
    \end{enumerate}
\end{example}
\begin{definition}
    A random variable is \textbf{discrete} if its range is countable.
\end{definition}
For a discrete random variable, the \textbf{probability mass function} is $p:\R\to\R$ defined by
\[p(x)=\begin{cases} 0&\text{if $x$ is not taken by $X$}\\\Pr(X=x_i)&\text{$x=x_i$ is taken by $X$}\end{cases}\]
In the example $\Pr(Y=0)=\frac{1}{8}$, $\Pr(Y=1)=\frac{3}{8}$, $\Pr(Y=2)=\frac{3}{8}$, $\Pr(Y=3)=\frac{1}{8}$.
Note that $\sum\limits_{i=1}^\infty p(x_i)=1$.

In the dice example, $\Pr(Z=k)=\left(\frac{5}{6}\right)^{k-1}\frac{1}{6}$ and indeed the geometric series sums to 1.
\begin{example}
    Each item can be one of $N$ different types, with $1/N$ chance independently of other items.
    We wish to collect all types.
    Let $X$ denote the number of items needed to collect all types.
    We wish to determine the mass funtion for $X$.

    We wish to find $\Pr(X>n)$ for all $n$.
    Then $\Pr(X=n)=\Pr(X>n-1)-\Pr(X>n)$.
    Now $\{X>n\}=A_1^{(n)}\cup\cdots\cup A_k^{(n)}$ where $A_k^{(n)}$ is the event that type $k$ has not been collected in $n$ items.
    Now
    \begin{align*}
        \Pr(X>n) &= \Pr(A_1\cup A_2\cup\cdots\cup A_N)\\
                 &= \sum\limits_{r=1}^n(-1)^{r+1}\binom{N}{r}\Pr(A_1\cap A_2\cap\cdots\cap A_r)\\
                 &= \sum\limits_{r=1}^n(-1)^{r+1}\binom{N}{r}\frac{(N-r)^n}{N^n}\\
    \end{align*}
\end{example}
\subsection{Expected Value}
\begin{definition}
    The \textbf{expected value} of a discrete random variable $X$ is given by $\E(X)=\sum\limits_{k=1}^\infty x_k\Pr(X=x_k)$.
\end{definition}
\begin{example}
    Consider two games:
    \begin{enumerate}
        \item Flip a fair coin, if H get \$100 and if T, lose
        \item Roll a fair die, if 6 get \$x, otherwise, go home.
    \end{enumerate}
    Let $X$ denote the gain if the order is AB.
    We have
    \[\Pr(X=0)=\frac{1}{2},\quad\Pr(X=100)=\frac{1}{2}\cdot\frac{5}{6},\quad\Pr(X=100+x)=\frac{1}{2}\cdot{1}{6}\]
    Let $Y$ denote the gain if the order is BA.
    We have
    \[\Pr(Y=0)=\frac{5}{6},\quad\Pr(Y=x)=\frac{1}{6}\frac{1}{2},\quad\Pr(Y=100+x)=\frac{1}{2}\cdot\frac{1}{6}\]
    so
    \[\E(X)=0\cdot\frac{1}{2}+100\cdot\frac{5}{12}+(100+x)\cdot\frac{1}{12}>\E(Y)=0\frac{1}{6}+x\cdot\frac{1}{12}+(x+100)\frac{1}{12}\]
    which reduces to $500>x$.
\end{example}
\begin{example}
    Note that $\E(X)=\sum\limits_{k=1}^\infty x_k\Pr(X=x_k)$ if the series is absolutely convergent.
    For example, define $\Pr(X=k)=\frac{1}{k(k+1)}$, which sums to 1, but
    \[\E(X)=\sum\limits_{k=1}^\infty\frac{1}{k+1}\]
    is infinite.
    But now, consider $Y$ with $\Pr(Y=0)=1/3$.
\end{example}
\begin{proposition}
    $\E(g(X))=\sum\limits_{k=1}^\infty g(x_k)\Pr(X=x_k)$
\end{proposition}
\begin{proof}
    Let $x_1,x_2,\ldots$ denote the possible values of $X$, and $y_1,y_2,\ldots$ denote the possible values of $Y$.
    Then
    \begin{align*}
        \sum\limits_{k=1}^\infty g(x_k)\Pr(X=x_k) &= \sum\limits_{l=1}^\infty\sum\limits_{x_k:g(x_k)=y_l}g(x_k)\Pr(X=x_k)\\
                                                  &= \sum\limits_{l=1}^\infty y_l\sum\limits_{x_k:g(x_k)=y_l}\Pr(X=x_k)\\
                                                  &= \sum\limits_{l=1}^\infty y_l\Pr(Y=y_l)=\E(Y)
    \end{align*}
\end{proof}
\begin{proposition}
    $\E(aX+b)=a\E(X)+\E(b)$
\end{proposition}
\begin{proof}
    Follows from linearity of the sum.
\end{proof}
\subsection{Variance}
Consider two random variables defined by $\Pr(X=1)=1/2$ and $\Pr(X=-1)=1/2$ vs $\Pr(X=100)=1/2$ and $Pr(X=-100)=1/2$.
They both have expected value $0$, so we want a value to measure the typical amount of fluctuation about the expected value.
Let $X$ be a random variable and $\mu=\E(X)$.
\begin{definition}
    We define the \textbf{variance} as $\Var(X)=\E[(X-\mu)^2]$.
\end{definition}
Note that $(X-\mu)^2=X^2-2\mu X+\mu^2$.
Then
\begin{align*}
    \Var x &= \E((X-\mu)^2)\\
           &= \sum\limits_{k=1}^\infty(x_k-2\mu x_k+\mu^2)\Pr(X=x_k)\\
           &= \sum\limits_{k=1}^\infty x_k^2\Pr(X=x_k)-2\mu\sum\limits_{k=1}^\infty x_k\Pr(X=x_k)+\mu^2\sum\limits_{k=1}^\infty\Pr(X=x_k)\\
           &= \E(X^2)-(\E(X))^2
\end{align*}
\begin{proposition}
    $\Var(aX+b)=a^2\Var X$.
\end{proposition}
\begin{example}
    \begin{enumerate}
        \item Roll a fair die, so $X$ can take $1,2,\ldots,6$ each with probability $1/6$.
            Then
            \begin{align*}
                \E(X)&=1\cdot\frac{1}{6}+\cdots+6\frac{1}{6}=\frac{7}{2}\\
                \E(X^2)&= \frac{1}{6}(1^2+2^2+\cdots+6^2)=\frac{7\cdot 13}{6}\\
                \Var(X) &= \frac{35}{12}
            \end{align*}
        \item Consider $\eta=1$ with chance $p$ and $0$ with chance $1-p$.
            Then $\E(\eta)=p$ and $\E(\eta^2)=p$, so $\Var(\eta)=p-p^2=pq$.
    \end{enumerate}
\end{example}
\section{The Binomial Distribution}
\subsection{Basic Properties}
\begin{definition}
    The Binomial distribution has parameters $n\geq 1$ and $p\in(0,1)$.
    Then $X\sim\Binom(n,p)$ if $\Pr(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$.
\end{definition}
\begin{example}
    Consider a test consisting of 20 yes-no questions; you fail if you have 17 or less correct answers.
    \begin{itemize}[nolistsep]
        \item You know the correct answer with probability $5/7$
        \item You have the incorrect answer with probability $1/7$
        \item You guess with probability $1/7$.
    \end{itemize}
    On a single question, the probability that you are correct is $5/7+1/14=11/14$.
    Now
    \begin{align*}
        \Pr(\text{fail}) &= \Pr(X\leq 17) = 1-\Pr(X=20)-\Pr(X=19)-\Pr(X=18)\\
                         &= 1-\binom{20}{20}\left(\frac{11}{14}\right)^{20}-\binom{20}{19}\left(\frac{11}{14}\right)^{19}\left(\frac{3}{14}\right)-\binom{20}{18}\left(\frac{11}{14}\right)^{18}\left(\frac{3}{14}\right)^2\\
                         &\approx 0.8345
    \end{align*}
\end{example}
Suppose $X\sim\Binom(n,p)$.
Then
\begin{align*}
    \E(X) &= \sum\limits_{k=0}^nk\binom{n}{k}p^k(1-p)^{n-k}\\
          &= \sum\limits_{k=1}^n\binom{n}{k}\left.\frac{\d{}}{\d{t}}\right\rvert_{t=1}(t^k)p^k(1-p)^{n-k}\\
          &= \left.\frac{\d{}}{\d{t}}\right\rvert_{t=1}\left(\sum\limits_{k=0}^n\binom{n}{k}(tp)^k(1-p)^{n-k}\right)\\
          &= \left.\frac{\d{}}{\d{t}}\right\rvert_{t=1}\left((tp+1-p)^n\right)\\
          &= \left.\left(n(tp-p+1)^{n-1}\cdot p\right)\right\rvert_{t=1}=np
\end{align*}
We can also compute
\begin{align*}
    \E[X(X-1)] &= \sum\limits_{k=1}^n k(k-1)\binom{n}{k}p^k(1-p)^{n-k}\\
               &= \sum\limits_{k=2}^n\left.\frac{\d{}^2}{\d{t^2}}\right\rvert_{t=1}(t^k)\binom{n}{k}p^k(1-p)^{n-k}\\
               &=\left.\frac{\d{}^2}{\d{t^2}}\right\rvert_{t=1}\left(\sum\limits_{k=0}^n\binom{n}{k}(tp)^k(1-p)^{n-k}\right)\\
               &= \left.\frac{\d{}^2}{\d{t^2}}\right\rvert_{t=1}(tp+(1-p))^n\\
               &= n(n-1)(tp+(1-p))^{n-2}p^2\\
               &= n(n-1)p^2\\
               &= \E(X^2)-\E(X)\\
               &= \E(X^2)-np
\end{align*}
so that
\[\E(X^2)=np^2(n-1)+np\]
and
\[\Var(X)=\E(X^2)-\E(X)^2=n^2p^2-np^2+np-n^2p^2=np(1-p)\]
We thus say that $X=\eta_1+\eta_2+\cdots+\eta_n$ where $\eta_i=\begin{cases}1\text{ if trial $i$ is a success}\\0\text{ if trial $i$ fail}\end{cases}$.
Thus the standard deviation scales with order $\sqrt{n}$.

Now let $X\sim\Binom(n,p)$, so that
\[\frac{\Pr(X=k)}{\Pr(X=k-1)}=\frac{\binom{n}{k}p^k(1-p)^{nk}}{\binom{n}{k-1}p^{k-1}(1-p)^{n-k+1}}=\frac{(n-k+1)p}{k(1-p)}\]
so that
\begin{align*}
    1 <\frac{\Pr(X=k)}{\Pr(X=k-1)} &\Leftrightarrow k(1-p)<(n-k+1)p\\
                                   &\Leftrightarrow k<(n+1)p
\end{align*}
There are two cases: if $(n+1)p$ is not an integer, then $k_0=\lfloor(n+1)p\rfloor$ is the single value that has maximal weight, and if $(n+1)p$ is an integer, then both $k_0=(n+1)p$ and $k_0-1$ have maximal weight.
With further analysis, one can show for any $\epsilon>0$ fixed, p fixed, as $n\to\infty$,
\subsection{Bernoulli's Law of Large Numbers}
\[\Pr\left(\left\lvert\frac{X}{n}-p\right\rvert>\epsilon\right)=0\]
This is called Bernoulli's Law of Large Numbers.
This is effective for $X\sim\Binom(n,p)$: fix $p$, and let $n\to\infty$.
\begin{example}
    \begin{enumerate}[label=(\alph*)]
        \item Shoot at a target 10 times, and suppose we hit the target with $p=0.1$.
            Let $X^{(a)}$ denote the number of hits, so that $\Pr(X^{(a)}>1)\approx 0.264\ldots$.
        \item Shoot at a target 20 times, and suppose we hit the target with $p=0.05$.
        \item Shoot at a target 100 times, and suppose we hit the target with $p=0.01$.
            Let $X^{(c)}$ denote the number of hits, so that $\Pr(X^{(b)}>1)=0.26424\ldots$
    \end{enumerate}
\end{example}
\section{Poisson Distribution}
\subsection{Basic Properties}
\begin{definition}
    Let $\lambda>0$ be a parameter.
    Then $X\sim\Poi(\lambda)$ if it can take $0,1,2,3,\ldots$ and $\Pr(X=k)e^{-\lambda}\frac{\lambda^k}{k!}$.
\end{definition}
\begin{proposition}
    Let $n\to\infty$, $p=p(n)\to 0$ so that $np(n)\to\lambda$.
    Then for any $k\in\N$,
    \[\binom{n}{k}p^k(1-p)^{n-k}\to e^{-\lambda}\frac{\lambda^k}{k!}\]
\end{proposition}
\begin{proof}
    Recall that
    \[\lim\left(1+\frac{1}{n}\right)^n=e,\quad\lim\left(1-\frac{1}{n}\right)^n=e^{-1}\]
    Thus let $a(n)\to\infty$ such that $\lim n/a(n)=x$.
    Then
    \[\lim\left(1-\frac{1}{a(n)}\right)^n = \lim\left[\left(1-\frac{1}{a(n)}\right)^{a(n)}\right]^{\frac{n}{a(n)}}=e^{-x}\]
    Now note that $\lim (n-u)p(n)=\lambda$, $\lim(1-p(n))^{-k}=1$, and
    \[\lim(1-p)^n=\lim(1-p(n))^n=\lim\left(1-\frac{1}{1/p(n)}\right)^n=e^{-\lambda}\]
    so
    \begin{equation*}
        \lim\frac{1}{k!}n(n-1)\cdots(n-k+1)p\cdot p\cdots p (1-p)^{-k}(1-p)^n=e^{-\lambda}\frac{\lambda^k}{k!}
    \end{equation*}
\end{proof}
In words, many independent trials with small success rate can be approximated by the Poisson distribution.
\begin{example}
    Since ``poisson'' means fish in french, we have an example about fishing.
    A fisherman goes fishing every day.
    He says: ``on average, on 1/5 days, I do not catch any fish''.
    What is the chance that he atches at least two fish next time?

    Let $X$ count the number of fish on a particular occasion.
    There are many fish which move independently in the lake (maybe this assumption is not accurate, but we'll ignore that), and for any fixed fish, the chance of being caught is small.
    Thus we can assume $X\in\Poi(\lambda)$.
    Then $X^{(1)},X^{(2)},\ldots$ is the number of fish caught in consequtive occasions.
    Then we have
    \[\frac{|\{X^{(k)}=0|k=1,\ldots,n\}|}{n}\approx\frac{1}{5}\]
    so by the Bernoulli Law of Large Numbers,
    \[\Pr(X=0)=\frac{1}{5}\]
    so $e^{-\lambda}=1/5$.
    Then $\lambda=\ln 5$ so that
    \[\Pr(X\geq 2)=1-\Pr(X=0)-\Pr(X=1)=1-e^{-\lambda}-\lambda e^{-\lambda}\approx 0.48\]
\end{example}
If $X\sim\Poi(\lambda)$, then
\[\E(X)=\sum\limits_{k=0}^\infty k\cdot e^{-\lambda}\frac{\lambda^k}{k!}=\lambda e^{-\lambda}\sum\limits_{m=0}^\infty\frac{\lambda^m}{m!}=\lambda e^{-\lambda}e^{\lambda}=\lambda\]
Similarly,
\begin{align*}
    \E(X(X-1)) &= \sum\limits_{k=0}^\infty k(k-1)e^{-\lambda}{\lambda^k}=e^{-\lambda}{\lambda^2}\sum\limits_{k=2}^\infty\frac{\lambda^{k-2}}{(k-2)!}=\lambda^2
\end{align*}
so that $\E(X^2)=\lambda^2+\lambda$ and $\Var(X)=\lambda$.
\begin{example}
    How many chocolate chips should you add per muffin so that no more than 1\% of muffins have no chocolate chips in them?

    Let $X$ denote the number of chips in a given muffin is poisson distributed, so $X\sim\Poi(\lambda)$.
    As well, $\lambda=N\cdot 1/M$, so $1/M$ is the success rate (the probability that a given chip is in the given muffin).
    We need $\Pr(X=0)\leq 0.01$, so $e^{-\lambda}\leq 0.01$ and $\lambda\geq\ln 100\approx 4.6$.
\end{example}
\subsection{Poisson Process}
A stochastic process is a random phenomenon evolving in time.
For example, a point process is a random collection of points on $[0,+\infty)$.
There are three properties that characterize the Poisson process.
\begin{definition}
    Consider two integer valued random variables, say $X$ and $Y$.
    These are \textbf{independent} if for any $k,l\in\Z$, the events $\{X=k\}$ and $\{Y=l\}$ are independent.
\end{definition}
\begin{definition}
    Let $f,g$ be arbitrary functions with $g(h)\to 0$ as $h\to 0$.
    Then $f:\R^+\to\R$ is said to be $o(g)$ of $\lim_{h\to 0^+}\frac{f(h)}{g(h)}=0$
\end{definition}
We thus have the following properties characterizing the Poisson process with intensity $\lambda$.
\begin{enumerate}
    \item Let $I_1,I_2,\ldots,I_n$ be non-overlapping intervals in $[0,\infty)$.
        Let $X_k$ denote the number of points in $I_k$ for $k=1,\ldots,n$.
        Then the events $X_1,X_2,\ldots,X_n$ are independent.
    \item Homogenity.
        Let $I$ be an interval of length $h$.
        Then $\Pr(\text{there is exactly one point in $I$})=\lambda h+o(h)$.
    \item No accumulations.
        Let $I$ be an interval of length $h$.
        Then $\Pr(\text{there are at least two points in $I$})=o(h)$.
\end{enumerate}
\begin{proposition}
    Let a point process satisfy the above properties.
    Then if $I_t$ is an arbitrary interval of length $t$ and $N_t$ denotes the number of impacts with $I_t$, then $N_t\sim\Poi(\lambda t)$.
\end{proposition}
\begin{proof}
    We want to compute $\Pr(N_t=k)$.
    We will consider this using an additional parameter $n$.
    Without loss of generality, consider the interval $[0,t]$.
    Define
    \[I_i^{(n)}=\left[\frac{(i-1)t}{n},\frac{it}{n}\right),\quad i=1,\ldots,n\]
    Let $E_i^{(n)}$ denote the event in which there is exactly one point in $I_i^{(n)}$, and $D_i^{(n})$ is the event in which there are at least two points in $I_i^{(n)}$.
    Let $D=D^{(n)}=\bigcup\limits_{i=1}^n D_i^{(n}$.
    Then
    \[\Pr(D)\leq\sum\limits_{i=1}^n\Pr(D_i^{(n)})=n\cdot o(t/n)=\frac{o(t/n)}{t/n}\cdot t=o(1)\]
    which tends to $0$ as $n$ goes to infinity.
    But then
    \begin{align*}
        \Pr(N_t=k)&=\Pr(\{N_t=k\}\cap D)+\Pr(\{N_t=k\}\cap D^c)=o(1)+\binom{n}{k}p(n)^k(1-p)^{n-k}\\
                  &= e^{-\lambda t}\frac{(\lambda t)^k}{k!}
    \end{align*}
    as $n$ goes to infinity.
    Note that $\{N_t=k\}\cap D^c$ means that there are exactly $k$ out of the $E_1^{(n)},\ldots,E_n^{(n)}$ that occurr, i.e. there are $k$ out of $n$ independent trials.
    As well, $p(n)=\Pr(E_1^{(n)})=\lambda t/n+o(t/n)$, so $n\cdot p(n)=\lambda t+o(1)$.
\end{proof}
\begin{example}
    On average, there are two earthquakes per week.
    What is the probability that there are at least 3 earthquakes in the first two weeks?
    How much time elapses until the first earthquake occurrs?

    The intensity of the process is given by $\lambda=2$ per week, so in 2 weeks, $N_2\sim\Poi(4)$.
    Then $\Pr(N_2\geq 3)=1-\Pr(N_2\leq 2)=1-e^4-4e^{-4}-\frac{4^2}{2}e^{-4}$.

    Now let $T$ denote the time that elapses until the first earthquake.
    Fix some $t>0$.
    This amounts to computing $\Pr(T>t)$.
    $T$ is a continuous random variable, and note that
    \[\Pr(T>t)=\Pr(N_t=0)=e^{-\lambda t}\]
\end{example}
Here are two more constructions.
\begin{example}
    \begin{enumerate}
        \item Consider a Poisson process of intensity $\lambda$.
            Each point is colored white with probability $p$ and orange with probability $1-p$.
            The collection of orange points make a Poisson process with intensity $(1-p)\lambda$.
            This can be proven by considering a Poisson distributed random variable, and for each $k$, think of a collection of $k$ objects that you keep with probability $p$.
            Then the number of points you get is still Poisson distributed.
        \item If $X_1\sum\Poi(\mu_1)$ and $X_2\sim\Poi(\mu_2)$, then $X_1+X_2\sim\Poi(\mu_1+\mu_2)$.
    \end{enumerate}
\end{example}
\begin{example}
    Let $X$ denote the number of matches.
    We have $\Pr(X=0)=1-1+\frac{1}{2}-\frac{1}{3!}+\cdots\pm\frac{1}{N!}$ which tends to $1/e$ for large $k$.
    Now, for some $j<N$, we claim that
    \[\Pr(X=j)=\binom{N}{j}\Pr(\text{No matches for $N-j$ people})\cdot\frac{1}{N}\frac{1}{N-1}\cdots\frac{1}{N-j+1}\]
    and fix $j$, and consider the limit as $N$ goes to infinity, yielding $\frac{1}{j!}{e^{-1}}$.
    Thus
    \[\Pr(Y=j)=\frac{e^{-1}}{j!}\]
    To see why this works, let $A_i$ denote the event in which person $i$ is matched with her phone.
    We have $\Pr(A_j)=\frac{1}{N}$.
    However, $\Pr(A_1|A_2)=\frac{1}{N-1}$, so as $N\to\infty$, the events are almost pairwise independent.
\end{example}
\section{Additional Discrete Distributions}
\subsection{Geometric Distribution}
\begin{definition}
    A \textbf{geometric distribution} is a sequence of independent trials, with $p\in(0,1)$ success rate.
    Then $\Pr(X=k)=p(1-p)^{k-1}$.
\end{definition}
We certainly have $\sum\limits_{k=1}^\infty q^{k-1}p=1$.
Now let
\[g(q)=\frac{1}{1-p}=\sum\limits_{k=0}^\infty q^k\]
If $X\sim\Geom(p)$, then
\begin{align*}
    \E(X)&=\sum\limits_{k=0}^\infty kq^{k-1}p\\
         &= p\sum\limits_{k=1}^\infty\frac{\d{}}{\d{q}}q^k\\
         &= p\frac{\d{}}{\d{q}}\left(\sum\limits_{k=0}^\infty q^k\right)\\
         &= pg'(q)=\frac{1}{p}
\end{align*}
and by similar methods,
\[\E(X(X-1))=pqg''(q)=2\frac{(1-p)}{p^2}\]
so that
\[\E(X^2)=\frac{2q+p}{p^2}=\frac{1+q}{p^2}\]
and
\[\Var(X)=\frac{q}{p^2}\]
The geometric distribution also has the memoryless property.
Note $\Pr(X>k)=q^k$ and $\Pr(X>k+n)=q^{k+n}$ so that $\Pr(X>k+n|X>n)=q^k$.
\subsection{Negative Binomial Distribution}
\begin{definition}
    A \textbf{negative binomial distribution} has parameters $p\in(0,1)$, $r\geq 1$, and $X$ can take $r,r+1,\ldots$ where $\{X=k\}$ is the event in which success $r$ occurs at trial $k$.
\end{definition}
$\Pr(X=r)=p^r$, $\Pr(X=r+1)=rp^rq$.
In general,
\[\Pr(X=k)=\binom{k-1}{r-1}p^rq^{k-r}\]
\begin{example}
    This is Stefan Banach's matchbox problem.
    There are $N$ matches in two boxes.
    At every stage, he randomly takes a match from either box.
    What is the probability that when he runs out of matches in one of the matchboxes, there are exactly $k$ matches in the other box?

    Will do this later.
\end{example}
We have
\[\E(X)=\frac{r}{p},\quad\Var X=r\frac{q}{p^2}\]
\subsection{Hypergeometric distribution}
Consider parameters $N$, $M<N$, $n<N$ and there are $N$ balls, $M$ black, $N-M$ white.
We draw $n$ balls without replacement.
We want $\{X=k\}$ to be the event that there are exactly $k$ black balls in $n$ balls drawn.
Then
\[\Pr(X=k)=\frac{\binom{M}{k}\cdot\binom{N-M}{n-k}}{\binom{N}{n}}\]
One can compute
\[\E(X)M\cdot\frac{n}{M}\]
Let $X$ and $Y$ be arbitrary discrete random variables.
Then $\E(X+Y)=\E(X)+\E(Y)$.
\begin{proof}
    We have
    \begin{align*}
        \E(X+Y)&=\sum\limits_{k=1}^\infty\sum\limits_{l=1}^\infty(x_k+y_l)\Pr(X=x_k,Y=y_l)\\
               &=\sum\limits_{k=1}^\infty x_k\sum\limits_{l=1}^\infty\Pr(X=x_k,Y=y_l)+\sum\limits_{l=1}^\infty y_l\sum\limits_{k=1}^\infty\Pr(X=x_k,Y=y_l)\\
               &=\sum\limits_{k=1}^\infty x_k\Pr(X=x_k)+\sum\limits_{l=1}^\infty y_l\Pr(Y=y_l)\\
               &= \E(X)+\E(Y)
    \end{align*}
\end{proof}
Now suppose $X$ and $Y$ are independent.
Then
\begin{align*}
    \E(XY) &= \sum\limits_{k=1}^\infty\sum\limits_{l=1}^\infty x_ky_l\Pr(X=x_k,Y=y_l)\\
           &= \sum\limits_{k=1}^\infty\sum\limits_{l=1}^\infty x_ky_l\Pr(X=x_k)\Pr(Y=y_l)\\
           &= \left(\sum\limits_{k=1}^\infty x_k\Pr(X=x_k)\right)\left(\sum\limits_{l=1}^\infty y_l\Pr(Y=y_l)\right)\\
           &=\E(X)\E(Y)
\end{align*}
so, if $X$ and $Y$ are independent, then $\Var(X+Y)=\Var(X)+\Var(Y)$.
We can use this to compute the expected values in previous examples.
\begin{enumerate}
    \item Suppose $X\sim\Binom(n,p)$.
        Then $X=\eta_1+\eta_2+\cdots+\eta_n$ where $\eta_j=1$ if the trial $j$ is successful, and 0 otherwise.
        As well, $\E(\eta_j)=p$ and $\Var(\eta_j)=p(1-p)$ so that $\E(X)=np$ and $\Var(X)=np(1-p)$.
    \item Suppose $X\sim\operatorname{Hypergeometric}(N,M,n)$.
        Then $X=\eta_1+\eta_2+\cdots+\eta_M$ where $\eta_j=1$ if back ball $j$ is drawn, and $0$ otherwise.
        Then $\E(\eta_j)=\E(\eta_1)=\Pr(\eta_1=1)=\frac{n}{N}$ and the result follows.
        However, these events are not independent, so we can't do the same thing for the variance.
\end{enumerate}
Consider as well the negative binomial $\{X=k\}$ is the event that event $r$ occurs at trial $k$.
Let $T_i$ denote the number of trials after the $(j-1)^\text{st}$ success needed to obtain the $j^\text{th}$ success.
Then $T_j$ are all independent and $T_i\sum\Geom(p)$ and
\[\E(T_j)=\frac{1}{p},\quad\Var(T_j)=\frac{q}{p^2}\]
so that
\[\E(X)=\frac{r}{p},\quad\Var(X)=\frac{rq}{p^2}\]
\begin{example}
    Consider $M$ types, each item can be any of the types with $1/M$ chance independently.
    Then if $X$ denotes the number of items fo collect all the type, then $X=\sum Y_j$, where $Y_j$ is the number of items needed to collect type $j$.
    Since $Y_1,\ldots,Y_m$ are independent and $Y_j\sim\Geom((M-j+1)/M)$, we have
    \[\E(X)=\sum\limits_{j=1}^m\E(Y_j)=M\left(\frac{1}{M}+\frac{1}{M-1}+\cdots+1\right)\]
\end{example}
\begin{example}
    $M=5$ people baord the elevator on a $K+1=11$ storied building (on the first floor).
    Each of them picks a destination, one of the floors $2$ through $K+1$.
    Let $X$ denote the number of times the elevator stops.

    Define $\eta_j$ to be 1, if the elevator stops on floor $(j+1)$, and $0$ otherwise.
    Then
    \[\E(\eta_j)=\E(\eta_1)=1-\left(\frac{K-1}{K}\right)^M\]
    so that
    \[\E(X)=K\left(1-\left(\frac{K-1}{K}\right)^M\right)\]
\end{example}
\chapter{Continuous Random Variables}
\section{Cumulative Distribution Function}
\subsection{Properties of the CDF}
\begin{definition}
    Let $X$ be an arbitrary random variable.
    Then the \textbf{cumulative distribution function} of $X$ is $F_X:\R\to\R$, defined by
    \[F_X(x)=\Pr(\{X\leq x\})\]
\end{definition}
\begin{example}
    Let $X$ denote the number of heads when 3 fair coins are flipped.
    Then $\Pr(X=0)=\Pr(X=3)=1/8$ and $\Pr(X=1)=\Pr(X=2)=3/8$.
    \[F_X(x)=
        \begin{cases}
            0 & x<0\\
            \frac{1}{8} & 0\leq x<1\\
            \frac{1}{2} & 1\leq x<2\\
            \frac{7}{8} & 2\leq x<3\\
            1 & 3\leq x
        \end{cases}
    \]
\end{example}
In general, if $X$ is a discrete random variable with no limit points, then $F_X(x)$ is a step function.
\begin{example}
    Consider a Poisson process with intensity $\lambda$, and let $T$ denote the time that elapses until the first impact occurs.
    Then
    \[F_T=
        \begin{cases}
            \Pr(T\leq x)=0 &x\leq 0\\
            1-\Pr(T>x)=1-\Pr(\{N_x=0\})=1-e^{-\lambda x}
        \end{cases}
    \]
\end{example}
First note if $a\leq b$, then $\Pr(A<x\leq b)=F_X(b)-F_X(a)$.
\begin{proposition}
    For a cumulative distribution function, the following hold:
    \begin{enumerate}
        \item $F(x)$ is non-decreasing
        \item $\lim_{x\to+\infty}F(x)=1$
        \item $\lim_{x\to-\infty}F(x)=0$
        \item $F(x)$ is right-continuous.
            Write this notation as $F(x_0+0)=F(x_0)$.
    \end{enumerate}
    Furthermore, these properties characterize the cumulative distribution function: any function with these 4 properties is the distribution function of some random variable.
\end{proposition}
\begin{proof}
    \begin{enumerate}
        \item Obvious.
        \item By (1), it suffices to show that $\lim_{x\to+\infty}F(x)=\lim_{n\to\infty}F(n)$.
            Let $E_n=\{X\leq n\}$.
            Then $E_n\subset E_{n+1}$ and $\bigcup\limits_{n=1}^\infty=\Omega$, so
            \begin{equation*}
                \lim_{n\to\infty}F(n)=\lim_{n\to\infty}\Pr(E_n)=\Pr\left(\bigcup\limits_{n=1}^\infty E_n\right)=\Pr(\Omega)=1
            \end{equation*}
        \item Let $F_n=\{X\leq n-\}$, so $F_n\supset F_{n+1}$ and $\bigcap\limits_{n=1}^\infty F_n=\emptyset$.
            Then
            \begin{equation*}
                \lim_{n\to-\infty}F(n)=\lim_{n\to\infty}F(-n)=\lim_{n\to\infty}\Pr(E_n)=\Pr\left(\bigcup\limits_{n=1}^\infty E_n\right)=\Pr(\Omega)=1
            \end{equation*}
        \item By (1), we have
            \begin{align*}
                F(x_0+0) &= \lim_{n\to\infty}\Pr\left(\left\{X\leq x_0+\frac{1}{n}\right\}\right)\\
                    &=\lim_{n\to\infty}\Pr(B_n)\\
                    &= \lim_{n\to\infty}\Pr(B_n)=\Pr\left(\bigcap\limits_{n=1}^\infty B_n\right)\\
                    &= \Pr(X\leq x_0)\\
                    &= F(x_0)
            \end{align*}
            Note that $F(x_0-0)$ may be less than $F(x_0)$.
            In fact, $F(x_0)-F(x_0-0)=\Pr(X=x_0)$.
    \end{enumerate}
\end{proof}
\begin{definition}
    A random variable $X$ is \textbf{absolutely continuous} if there exists some $f_X:\R\to\R$ such that for any Borel set $V\subset\R$,
    \[\Pr(X\in V)=\int_V f(x)\d{x}\]
\end{definition}
In particular, $0=\int_{x_0}^{x_0}f(x)\d{x}=\Pr(\{x=x_0\})$.
As well, $f(x)\geq 0$ for all $x\in\R$ and $\int_{-\infty}^\infty f(x)\d{x}=1$.
Moreover,
\[F_X(x)=\Pr(X\leq x)=\int_{-\infty}^x f(t)\d{t}\]
so $f_X$ is an anti-derivative of $F_X$.
Furthermore,
\[F_X(b)-F_X(a)=\Pr(a\leq X<b)=\int_a^b f(x)\d{x}\]
Note that it is not only true that $F_X(x)$ is continuous; it is also differentiable almost everywhere, and $\frac{\d{F}}{\d{x}}=f(x)$ almost everywhere.
\begin{example}
    Recall
    \[F_T(t)=
        \begin{cases}
            0 &: t\leq 0\\
            1-e^{-\lambda t}: t>0
        \end{cases}
    \]
    and
    \[f_T(t)=
        \begin{cases}
            0 &: t<0\\
            \lambda e^{-\lambda t} &:t >0
        \end{cases}
    \]
\end{example}
If $X$ is absolutely continuous, then $\Pr(X=a)=0$.
Then the values of $f(x)$ not probabilities: consider $a\in\R$, $\epsilon\to 0$.
Then
\[\Pr(a\leq X\leq a+\epsilon)=\int_a^{a+\epsilon}f(x)\d{x}\approx f(a)\epsilon+o(\epsilon)\]
is the best linear approximation of the cumulative distribution at $a$.
\begin{definition}
    The expected value of $X$ is $\E(X)=\int_{-\infty}^\infty xf_X(x)\d{x}$, if the integral is absolutely convergent.
\end{definition}
\begin{proposition}
    $\E(Y)=\E(g(X))=\int_{-\infty}^\infty g(x)f_X(x)\d{x}$.
\end{proposition}
\begin{proof}
    Let $Y$ be a non-negative random variable with density $f_Y(y)$.
    Then we will see that $\E(Y)=\int_0^{\infty}\Pr(Y>y)\d{y}$.
    Note that
    \begin{align*}
        \int_0^\infty \Pr(Y>y)\d{y}&=\int_0^\infty \int_y^\infty f_Y(x)\d{x}\d{y}\\
                                   &=\iint_D H(x,y)\d{x}\d{y}\\
                                   &= \int_0^\infty\int_0^x f_Y(x)\d{y}\d{x}\\
                                   &= \int_0^\infty x f_Y(x)\d{x}\\
                                   &= \E(Y)
    \end{align*}
    Without loss of generality, assume $g(x)\geq 0$ (or write $g(x)=g_+(x)-g_-(x)$).
    Thus $Y=g(X)\geq 0$, so
    \begin{align*}
        \E(g(X)) &= \E(Y)\\
                 &= \int_0^\infty \Pr(Y>y)\d{y}\\
                 &= \int_0^\infty \Pr(g(X)>y)\d{y}\\
                 &= \int_{-\infty}^\infty \int_0^{g(x)}f_X(x)\d{y}\d{x}\\
                 &= \int_{-\infty}^\infty g(x)f_X(x)\d{x}
    \end{align*}
    For any $y_0>0$, $\{(x,y)\in U|y=y_0\}=D_{y_0}$.
    Thus $(x,y)\in U$ if and only if $g(x)>y>0$, so $U$ is the domain between the $x-$axis and the graph of $g$.
\end{proof}
Consider the Cantor function.
Suppose this function had a density function $f(x)$, we would have that $F'(x)=f(x)$ for almost every $x$.
Conversely, $F'(x)=0$ almost everywhere.
\section{Important Absolutely Continuous Distributions}
\subsection{Uniform Distribution}
We say $X\sim\operatorname{UNI}[\alpha,\beta]$ for $\alpha<\beta$ if
\[f_X(x)=
    \begin{cases}
        \frac{1}{\beta-\alpha} &:\alpha\leq x\leq\beta\\
        0 &:\text{otherwise}
    \end{cases}
\]
so that
\[F_X(x)=
    \begin{cases}
        0 &:x <\alpha\\
        \frac{x-\alpha}{\beta-\alpha} &:\alpha\leq x\leq\beta\\
        1 &:x >\beta
    \end{cases}
\]
Now if $Y\sim\operatorname{UNI}[0,1]$, then
\[\E(Y)=\int_0^1 x\d{x}=\frac{1}{2},\quad\E(Y^2)=\int_0^1 x^2\d{x}=\frac{1}{3}\]
so that
\[\Var(Y)=\frac{1}{3}-\frac{1}{4}=\frac{1}{12}\]
and in general,
\[\E(X)=(\beta-\alpha)\E(Y)+\alpha=\frac{\beta+\alpha}{2}\]
\[\Var(X)=(\beta-\alpha)^2\Var(Y)=\frac{(\beta-\alpha)^2}{12}\]
\begin{example}
    Buses leave from every station every 15 minutes, starting at 7am.
    What is the chance that I have to wait at least 10 minutes if
    \begin{enumerate}
        \item My arrival time at the station is uniform between 7am and 8am?
    \end{enumerate}
\end{example}
\subsection{Standard Normal Distribution}
We say $Z$ is standard normal if $Z\sim N(0,1)$ if it has density
\[\phi(x)=\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\]
Then its cumulative distribution function is given by
\[\Phi(x)=\int_{-\infty}^x f(t)\d{x}\]
cannot be expressed in terms of elementary functions.
However, we do have $\Phi(0)=1/2$, and $\Phi(-x)=1-\Phi(x)$.
Let's show that $\int_{-\infty}^\infty f(x)\d{x}=1$.
We have
\begin{align*}
    I^2 &= \int_{-\infty}^\infty e^{-x^2/2}\d{x}\cdot\int_{-\infty}^\infty e^{-y^2/2}\d{y}\\
        &= \iint_{\R^2}e^{-(x^2+y^2)/2}\d{x}\d{y}\\
        &= \int_0^\infty\int_0^{2\pi}e^{-r^2/2}r\d{\theta}\d{r}\\
        &= 2\pi\int_0^\infty re^{-r^2/2}\d{r}=2\pi
\end{align*}
as reqired.
Furthermore, $\E(Z)=0$ since $xf(x)$ is an odd function.
As well,
\begin{align*}
    \Var(Z) &= \E(Z^2)=\int_{-\infty}^\infty x^2 f(x)\d{x}\\
            &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty x^2e^{-x^2/2}\d{x}\\
            &= \frac{1}{\sqrt{2\pi}}\frac{1}{\sqrt{2\pi}}\left(\left[-xe^{-x^2/2}\right]_{0}^\infty+\int_{-\infty}^\infty e^{-x^2}\d{x}\right)\\
            &= \frac{1}{\sqrt{2\pi}}\cdot\sqrt{2\pi}=1
\end{align*}
\begin{definition}
    A random variable $\eta$ is standard if $\E(\eta)=0$ and $\Var(\eta)=1$.
\end{definition}
Let $X$ be an arbitrary random variable with $\mu=\E(X)$ and $\sigma^2=\Var(X)$.
Then $Y=(X-\mu)/\sigma$ is standard.
In particular, $Z$ is standard normal if $f_Z(x)=f(x)+\frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ and $F_Z(x)=\Phi(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^{-t^2/2}\d{t}$.
\begin{definition}
    Let $\mu\in\R$, $\sigma>0$ be fixed parameters.
    Then $X\sim\mathcal{N}(\mu,\sigma^2)$ if $X=\sigma Z+\mu$, with $Z\sim\mathcal{N}(0,1)$.
\end{definition}
\begin{example}
    The amount of beer in a class is normally distributed with expected value 0.5l with standard deviation 3cl.
    What is the chance that you get less than 4.5dl?
\end{example}
\begin{solution}
    Let's use dl everywhere.
    We have $X\sim\mathcal{N}(5,0.09)$ since $\sigma=0.3$.
    Then
    \[\Pr(X\leq 4.5)=\Pr\left(\frac{X-\mu}{\sigma}\leq\frac{4.5-5}{0.3}\right)=\Phi(-1.66)=1-\Phi(1.66)\approx 0.0485\]
\end{solution}
\begin{theorem}[deMoivre-Laplace]
    Let $X\sim\Binom(n,p)$ with $p$ fixed and $n\to\infty$ and fix $a<b$.
    Then
    \[\Pr\left(a<\frac{X-np}{\sqrt{np(1-p)}}\leq b\right)\underset{n\to\infty}{\longrightarrow}\Phi(b)-\Phi(a)\]
\end{theorem}
\begin{proof}[Stirling's Formula]
    For any $k\geq 1$,
    \[0\leq\int_k^{k+1}\ln(x)\d{x}-\frac{\ln k+\ln(k+1)}{2}\leq\frac{1}{k^2}\]
    which follows by the Lagrange remainder.
    We then have
    \[0\leq\int_1^N\ln(x)\d{x}-\sum\limits_{k=1}^{N-1}\frac{\ln k+\ln(k+1)}{2}\leq\sum\limits_{k=1}^{N-1}\frac{1}{k^2}\]
    where $\int_1^N\ln x\d{x}=N\ln N-N+1$.
    As well,
    \begin{align*}
        \sum\limits_{k=1}^{N-1}\frac{\ln k+\ln(k+1)}{2} = \ln(N!)-\ln(\sqrt{N})
    \end{align*}
    so $\ln(N^N)-\ln(e^N)-\ln(N!)+\ln(\sqrt{N})\to C$ as $N$ goes to infinity.
\end{proof}
\begin{example}
    Flip a coin 40 times.
    What is the probability that there are 20 heads?
    
    We have $X\sim\Binom(40,1/2)$, $\E(X)=20$ and $\Var(X)=10$.
    Then
    \begin{align*}
        \Pr(X=20) &= \Pr(19.5<X\leq 20.5)\\
                  &= \Pr\left(\frac{19.5-20}{\sqrt{10}}<\frac{X-20}{\sqrt{10}}\leq\frac{20.5-20}{\sqrt{10}}\right)\\
                  &= \Pr\left(\frac{-0.5}{\sqrt{10}}<Z\leq\frac{0.5}{\sqrt{10}}\right)\\
                  &= 2\Phi\left(\frac{0.5}{\sqrt{10}}\right)-1\\
                  &= 0.1272
    \end{align*}
    and the true value is approximately $0.1254$.
\end{example}
\begin{example}
    There is a class with 400 students.
    Every student shows up at the lecture with 0.6 chance.
    How many seats are needed to ensure that everybody who shows up can sit down with 99\% chance?

    Let $X$ denote the number of students who show up, so $X\sim\Binom(400,0.6)$.
    Then $\E(X)=240$ and $\sigma(X)=\sqrt{400\cdot 0.6\cdot 0.4}=\sqrt{96}$.
    We want
    \[0.99\leq\Pr(X\leq a)=\Pr\left(\frac{X-240}{9.8}\leq\frac{9-240}{9.8}\right)=\Phi\left(\frac{a-240}{9.8}\right)\]
    and since $\Phi(2.33)\approx 0.9901$, we want $a-240\geq2.33\cdot 9.8$ and $a\geq\approx 2.63$.
\end{example}
\begin{example}
    How many times does a fair coin have to be flipped to ensure that the proportion of heads is between 0.49 and 0.51 with 95\% chance?

    Let $X$ denote the number of heads, with $X\sim\Binom(n,1/2)$.
    We want
    \begin{align*}
        0.95&\leq\Pr(0.49<X/n\leq 0.51)\\
            &=\Pr(0.49n\leq X\leq 0.51n)\
            &= \Pr\left(\frac{0.49n-0.51n}{\sqrt{n}/2}\leq\frac{X-n/2}{\sqrt{n}/2}\leq\frac{0.01n}{\sqrt{n}/2}\right)\\
            &= \Phi(0.02\sqrt{n})-(1-\Phi(0.02\sqrt{n}))
    \end{align*}
    i.e. $\Phi(1.96)\approx 0.975\leq\Phi(0.02\sqrt{n})$, in other words that $1.96\leq 0.02\sqrt{n}$ and $n\geq 9604$.
\end{example}
\[0.96\leq \Pr\left(\left\lvert\frac{X}{n}-p\right\rvert\leq 0.02\right)=\cdots\]
will get $Cp(1-p)\leq n$, and this holds for $C/4\leq n$.
\subsection{Exponential Distribution}
Let $\lambda>0$.
Then $X\sim\Exp(\lambda)$ if its density is
\[f_X(x)=\begin{cases}0 &:x<0\\\lambda e^{-\lambda x} &: x\geq 0\end{cases}\]
We thus have
\begin{align*}
    \E(X^k) &= \int_0^\infty x^k\lambda e^{-\lambda x}\d{x}\\
            &= \frac{k}{\lambda}\int_0^\infty x^{k-1}e^{-\lambda x}\d{x}\\
            &\quad\vdots\\
            &= \frac{k!}{\lambda^k}
\end{align*}
In particular, $\Var(X)=\E(X^2)-\E(X)^2=1/\lambda^2$ so $\sigma=1/\lambda$.
This has the memoryless property: if $X\sim\Exp(\lambda)$, and $t,s>0$, then $\Pr(X>t)=1-\Pr(X\leq t)=e^{-\lambda t}$.
Then
\[\Pr(X>t+s|X>s) = \frac{\Pr(X>s+t\cap X>s)}{\Pr(X>s)}=e^{-\lambda t}\]
Let $X$ be a random variable with density $f_X(x)$ and distribution function $F_X(x)$.
Let $Y=k(X)$ for some function $k:\R\to\R$.
\begin{proof}
    Suppose $k(x)$ is increasing.
    Then $F_Y(y)=\Pr(Y\leq y)=\Pr(k(X)\leq y)=\Pr(X\leq k^{-1}(y))=F_X(k^{-1}(y))$.
    By differentiation, $f_Y(y)=f_X(k^{-1}(y))\frac{\d{}}{\d{y}}(k^{-1}(y))$.
    The case when $k$ is decreasing is analgous.
\end{proof}
\begin{example}
    Suppose $H\sim\UNI[-\pi/2,\pi/2]$ and $k(x)=\tan(x)$.
    Then
    \[f_X(x)=f_H(\tan^{-1}(x))\cdot\left\lvert\frac{\d{}}{\d{x}}\tan^{-1}(x)\right\rvert=\frac{1}{\pi}\cdot\frac{1}{1+x^2}\]
    This distribution is called the standard Cauchy distribution and its expected value does not exist.
\end{example}
\begin{definition}
    The joint distribution (cumulative) function of $X$ and $Y$ is $F:\R^2\to\R$ by
    \[F(x,y)=\Pr(X\leq x,Y\leq y)\]
\end{definition}
Note that $\lim_{y\to\infty}F(x,y)=F_X(x)$, the marginal distribution function of $X$.
Similarly, $\lim_{y\to-\infty}F(x,y)=0$ for any fixed $x\in\R$.
We also require $\Pr(a_1<X\leq b_1,a_2<Y\leq b_2)=F(b_1,b_2)-F(a_1,b_2)-F(b_1,a_2)+F(a_1,a_2)\geq0$ as a probability on any rectangle.
\begin{definition}
    Let $X,Y$ be discrete random variables.
    Then $p(x_k,y_l)=\Pr(X=x_k\cap Y=y_l)$ (and 0 otherwise).
\end{definition}
\begin{example}
    Suppose there are 2 white, 2 red, and 1 blue ball in a box.
    Draw 2 balls, and let $X$ count the number of red and $Y$ count the number of blue.
    Then $p(x,y)$ is given by
    \begin{center}
        \begin{tabular}{c|c|c|c|c}
            $X,Y$&0&1&2\\
            \hline
            0&$\frac{1}{10}$&$\frac{4}{10}$&$\frac{1}{10}$&$\frac{6}{10}$\\
            \hline
            1&$\frac{2}{10}$&$\frac{2}{10}$&$0$&$\frac{4}{10}$\\
            \hline
            &$\frac{3}{10}$&$\frac{6}{10}$&$\frac{1}{10}$&
        \end{tabular}
    \end{center}
\end{example}
\begin{definition}
    We say $X$ and $Y$ are jointly absolutely continuous if there exists $f:\R^2\to R$ so that $\Pr((X,Y)\in D)=\iint_D f(x,y)\d{x}\d{y}$ for any Borel set $D\subset\R^2$.
\end{definition}
We thus have
\[F(a_1,a_2)=\int_{-\infty}^{a_1}\int_{-\infty}^{a_2}f(x,y)\d{x}\d{y}\]
so that
\[\frac{\partial F}{\partial x}=\int_{-\infty}^{a_2}f(a_1,y)\d{y},\qquad\frac{\partial F^2}{\partial x\partial y}(x,y)=f(x,y)\geq 0\]
If $(X,Y)$ are jointly absolutely continuous, then $X$ is absolutely continuous and $Y$ is absolutely continuous.
Furthermore,
\[f_X(x)=\int_{-\infty}^\infty f(x,y)\d{y}\]
However, the converse can fail to hold:
\begin{example}
    Say $X$ is an arbitrary absolutely continuous variable, and $Y\equiv X$.
    But then $(X,Y)=(X,X)$ is restricted to a measure 0 subset, so no such density function $f(x,y)$ exists.
\end{example}
\begin{example}
    Suppose $X$ and $Y$ have joint density
    \[f(x,y)=
        \begin{cases}
            2e^{-x}e^{-2y} &: x,y>0\\
            0 & \text{otherwise}
        \end{cases}
    \]
    Then
    \[f_X(x)=\int_{-\infty}^\infty f(x,y)\d{y}=
        \begin{cases}
            0 &: x\leq 0\\
            e^{-x} &: x>0
        \end{cases}
    \]
    and
    \[f_X(y)=\int_{-\infty}^\infty f(x,y)\d{y}=
        \begin{cases}
            0 &: x\leq 0\\
            2e^{-y} &: x>0
        \end{cases}
    \]
    We can also compute
    \begin{align*}
        \Pr(X\geq Y) &= \int_{-\infty}^\infty \int_{-\infty}^x f(x,y)\d{y}\d{x}\\
                     &= \int_0^{\infty}\int_0^x 2e^{-2y}e^{-x}\d{y}\d{x}\\
                     &= \int_0^\infty (-e^{-3x}+e^{-x})\d{x}\\
                     &= \frac{2}{3}
    \end{align*}
\end{example}
\begin{definition}
    Let $D$ be a domain in $\R^2$ with finite measure.
    Then $(X,Y)$ is uniformly distributed on $D$ if their joint density satisfies
    \[f(x,y)=
        \begin{cases}
            0 &: (x,y)\notin D\\
            \frac{1}{\mu(D)} &: (x,y)\in D
        \end{cases}
    \]
\end{definition}
\begin{example}
    Let $D$ denote the triangle with vertices $(0,0)$, $(3,0)$, and $(0,1)$.
    Then
    \[f(x,y)=
        \begin{cases}
            \frac{2}{3}&: (x,y)\in D\\
            0 &:\text{otherwise}
        \end{cases}
    \]
    so that
    \[f_X(x)=\int_{-\infty}^\infty f(x,y)\d{y}=
        \begin{cases}
            0 &: x\notin[0,3]\\
            \frac{2}{3}\left(1-\frac{x}{3}\right) &: x\in[0,3]
        \end{cases}
    \]
    \[f_X(x)=\int_{-\infty}^\infty f(x,y)\d{y}=
        \begin{cases}
            0 &: y\notin[0,1]\\
            2-2y &: y\in[0,1]
        \end{cases}
    \]
\end{example}
\begin{definition}
    $X$ and $Y$ are independent if for any pair of intervals $I,J$,
    \[\Pr(X\in I,Y\in J)=\Pr(X\in I)\cdot\Pr(Y\in J)\]
    or equivalently, $F(x,y)=F_X(x)\cdot F_Y(y)$.
\end{definition}
In general, $(X,Y)$ uniform on $D$ are not independent unless $D$ is a cartesian product of subsets of $\R$.
\begin{example}
    Consider $f(x,y)=Axy$ whenever $x+y\leq $ and $x,y\in[0,1]$, and 0 otherwise.
    Then
    \[f_X(x)=
        \begin{cases}
            0 &: x\leq 0, x\geq 1\\
            \int_0^{1-x}Axy\d{y}=Ax(1-x)^2/2
        \end{cases}
    \]
    so that $A=24$.
    \[f_Y(y)=
        \begin{cases}
            0 &: y\leq 0, y\geq 1\\
            \int_0^{1-y}Axy\d{y}=Ay(1-y)^2/2
        \end{cases}
    \]
\end{example}
\begin{example}
    A woman and a man agree to meet at some location between noon and 1pm.
    Both arrive at a random time, uniformly distributed within this hour independently.
    What is the probability that the one who arrives first has to wait more than 10 minutes?

    Let $X\sim\operatorname{UNI}[0,60]$, $Y\sim\operatorname{UNI}[0,60]$.
    Then $\Pr(|X-Y|>10)=\frac{50^2}{60^2}$
\end{example}
\begin{example}
    Consider a table ruled with parallel equidistant lines (2cm apart), on which we flip a needle of length 1cm.
    If $X$ is the distance of the midpoint of the needle to the closest line, then $X\sim\operatorname{UNI}[0,1]$.
    Let $H$ denote the angle that the needle makes with the horizontal.
    Then
    \[f(\theta,x)=\frac{2}{\pi}\]
    whenever $0<x<1$ and $0<\theta<\pi/2$ so that
    \begin{align*}
        \Pr(\text{intersection}) &= \Pr(X<\frac{1}{2}\cos\theta)\\
                                 &= \iint_D f(x,\theta)\d{x}\d{\theta}\\
                                 &= \frac{2}{\pi}\int_0^{\pi/2}\int_0^{\cos\theta/2}\d{x}\d{\theta}\\
                                 &= \frac{1}{\pi}\int_0^{\pi/2}\cos\theta\d{\theta}\\
                                 &= \frac{1}{\pi}
    \end{align*}
\end{example}
Let's now consider sums of independent random variables.
If $X$ and $Y$ are independent with known distributions, what is the distribution of $X+Y$?
If $X,Y$ are discrete and integer valued,
\begin{align*}
    \Pr(X+Y=k) &= \sum\limits_{l=-\infty}^\infty \Pr(X+Y=k|X=l)\Pr(X=l)\\
               &= \Pr(Y=k-l)\Pr(X=l)
\end{align*}
Suppose $X$ has density $f_X(x)$ and $Y$ has density $f_Y(y)$.
What can we say about the distribution of $Z=X+Y$?
We have $f(x,y)=f_X(x)f_Y(y)$ so that
\begin{align*}
    F_Z(z) &= \Pr(Z\leq z)\\
           &=\Pr(X+Y\leq z)\\
           &= \iint_{H}f(x,y)\d{x}\d{y}\\
           &= \int_{-\infty}^\infty \int_{-\infty}^{z-x}f_X(x)f_Y(y)\d{y}\d{x}\\
           &= \int_{-\infty}^\infty f_X(x)\left(\int_{-\infty}^{z-x}f_Y(y)\d{y}\right)\d{x}
\end{align*}
and
\[f_Z(z)=\frac{\d{F_z}}{\d{z}}(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\d{x}\]
Suppose $X$ and $Y$ are independentt with densities $f_X(x)$ and $f_Y(y)$ respectively.
Write $Z=X+Y$ and it has density
\[f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\d{x}\]
\begin{example}
    Suppose $X,Y\sim\operatorname{UNI}[0,1]$.
    Then
    \[f_Z(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\d{x}=
        \begin{cases}
            0 &: z\leq 0,z\geq 2\\
            z &: 0<z\leq 1\\
            2-z & 1<z<2
        \end{cases}
    \]
\end{example}
\begin{lemma}
    Let $X\sim\mathcal{N}(0,1)$ and $Y\sim\mathcal{N}(0,\sigma^2)$ are independent, then $X+Y\sim\mathcal{N}(0,1+\sigma^2)$.
\end{lemma}
\begin{proof}
    First recall that $\E(X+Y)=\E(X)+\E(Y)$ and since $X,Y$ are independent, $\E(X\cdot Y)=\E(X)\cdot\E(Y)$ so $\Var(X+Y)=\Var(X)+\Var(Y)$.
    Thus it suffices to check that $X+Y$ is normally distributed.
    First note that
    \[f_X(x)f_Y(z-x)=C_0e^{-x^2/2}e^{-(z-x)^2/(2\sigma^2)}=C_0e^{-(Ax^2+Bzx+Dz^2)/2}\]
    so we have
    \begin{align*}
        f_Z(z) &= C_0e^{-Ez^2}\int_{-\infty}^\infty e^{-\frac{1}{2}A(x-Fz)^2}\d{x}\\
               &= C_0e^{-Ez^2}\int_{-\infty}^\infty e^{-\frac{A}{2}A(\tilde x)^2}\d{\tilde x}\\
               &= C_0C_1e^{-Ez^2}
    \end{align*}
    is normally distributed.
\end{proof}
Consider $x_0,y_0$ fixed and $\d{y}$ small.
Then
\begin{align*}
    \lim_{h\to 0}\Pr(X\leq x_0|y_0<y\leq y_0+h) &= \lim_{h\to 0}\frac{\Pr(X\leq x_0\cap Y\leq y_0+h)-\Pr(X\leq y_0\cap Y\leq y_0)}{\Pr(Y\leq y_0+h)-\Pr(Y\leq y_0)}\\
                                                &= \lim_{h\to 0}\frac{F(x_0,y_0+h)-F(x_0,y_0)}{F_Y(y_0+h)-F_Y(y_0)}\\
                                                &= \lim_{h\to 0}\frac{\frac{F(x_0,y_0+h)-F(x_0,y_0)}{h}}{\frac{F_Y(y_0+h)-F_Y(y_0)}{h}}\\
                                                &= \frac{\frac{\partial F}{\partial y}(x_0,y_0)}{F'_Y(y_0)}\\
                                                &= \frac{\int_{-\infty}^{x_0}\frac{\partial^2F}{\partial x\partial y}(t,y_0)\d{t}}{f_Y(y_0)}\\
                                                &= \int_{-\infty}^{x_0}\frac{f(x,y)}{f_Y(y_0)}\d{y}
\end{align*}
This inspires the following definition:
\begin{definition}
    Let $y_0\in\R$ be such that $f_Y(y_0)>0$.
    Then the conditional density of $X$, given $Y=y_0$, is
    \[f_{X|Y}(X|Y=y_0)=\frac{f(x,y_0)}{f_Y(y_0)}\]
\end{definition}
\begin{example}
    Let $f(x,y)$ be uniform on the triangle with vertices $(0,0)$, $(0,1)$, and $(1,0)$.
    Determine the conditional distribution of $Y$ given some $X=x_0$.

    Firstly, we have
    \[f_X(x_0)=\int_{-\infty}^\infty f(x_0,y)\d{y}=\begin{cases}\int_0^{x_0}2\d{y}=2x_0 &:0<x_0<1\\0&:\text{otherwise}\end{cases}\]
    so that
    \begin{align*}
        f_{Y|X}(y|X=x_0)=\frac{f(x_0,y)}{f_X(x_0)}=\frac{f(x_0,y)}{2x_0}=
        \begin{cases}
            \frac{1}{x_0} &: 0<y<x_0\\
            0 &:\text{otherwise}
        \end{cases}
    \end{align*}
\end{example}
If $(X,Y)$ are jointly uniformly distributed on some domain $D$, then the conditional distributions are always uniform on the relevant intervals.
The marginal distributions are, generally, not uniform.
\begin{example}
    Let $X\sim\operatorname{UNI}[0,1]$ and $Y|X=x_0\sim\operatorname{UNI}[0,x_0]$.
    Then
    \[f_{Y|X}(y|X=x)=\begin{cases}\frac{1}{x} &:0<y<x\\0\text{otherwise}\end{cases}\]
    so that
    \begin{align*}f(x,y) &= f_{Y|X=x}(y|X=x)\cdot f_X(x)=\begin{cases}\frac{1}{x} &:0<x<1,0<y<x\\0&:\text{otherwise}\end{cases}\end{align*}
    Then
    \[f_Y(y)=\int_{-\infty}^\infty f(x,y)\d{x}=\begin{cases}\int_y^1\frac{1}{x}\d{x}=-\ln(y)&:0<y<1\\0\text{otherwise}\end{cases}\]
\end{example}
\section{Expectation}
Let $f(x,y)$ be the joint density function for $(X,Y)$.
Then $\E(g(X,Y))=\iint g(x,y)f(x,y)\d{x}\d{y}$.
Suppose $g(x,y)$ is uniformly distributed 
\begin{align*}
    \E(|X-Y|) &= \iint_{[0,60]^2}\frac{|x-y|}{60^2}\d{x}\d{y}\\
              &= 2\int_0^{60}\int_0^x\frac{x-y}{60^2}\d{y}\d{x}\\
              &= \frac{1}{60^2}\int_0^{60}\left[xy-\frac{y^2}{2}\right]_{y=0}^{y=x}\d{x}\\
              &= \frac{2}{60^2}\int_0^{60}\frac{x^2}{2}\d{x}\\
              &= 20
\end{align*}
As an application, suppose $X$ is hypergeometric where $\Pr(X=k)=\frac{\binom{M}{k}\cdot\binom{N-M}{n-k}}{\binom{N}{n}}$.
\begin{example}
    Suppose there are $N$ hunters waiting for a flock of $K$ ducks to fly by.
    Each hunter picks a duck at ranom and hits it with probability $p$.
    Let $X$ count the number of ducks which survive.
    What is $\E(X)$?

    Let $\eta_i=1$ if duck $i$ survives, and 0 otherwise.
    Then $\E(\eta_i)=\left(1-\frac{p}{K}\right)^N$ for all $i$.
    Thus $\E(X)=K\E(\eta_i)$.
\end{example}
\begin{example}
    Suppose $X\sim\operatorname{Neg.Binom}(r,p)$.
    Then $\Pr(X=k)=\binom{k-1}{r-1}p^rq^{k-r}$ for each $k=r,r+1,\ldots$.
    Let $T_j$ denote the number of trials in which success $j$ happens after turn $(j-1)$, so $T_j\sim\operatorname{Geom}(P)$.
    Then $\E(T_j)=1/p$ and $\E(X)=r/p$.
\end{example}
\begin{example}
    Suppose we have $M$ ``$A$'' symbols and $N$ ``$B$'' symbols, where any of the $(M+N)!$ arrangements are equally likely.
    Let $R$ denote the number of runs.
    What is $\E(R)$?

    Write $R=X+Y$ where $X$ counts the number of ``$A$'' runs, and $Y$ the number of ``$B$'' runs.
    Let $X=X_1+X_2+\cdots+X_{M+N}$ where
    \[X_k=
        \begin{cases}
            1 &\text{if an $A$ run starts at position $k$}\\
            0 &\text{otherwise}
        \end{cases}
    \]
    We have
    \[\E(X_1)=\Pr(\text{first symbol $A$})=\frac{M}{M+N}\]
    and for $k\geq 2$
    \[\E(X_k)=\Pr(\text{symbol $k-1$ is $B$, symbol $k$ is $A$})=\frac{N\cdot m}{(M+N)(M+N-1)}\]
    Thus
    \begin{align*}
        \E(R) &= \E(X)+\E(Y)\\
              &= \sum\limits_{k=1}^{M+N}\E(X_k)+ \sum\limits_{k=1}^{M+N}\E(Y_k)\\
              &= \E(X_1)+(M+N-1)\E(X_k) + \E(Y_1)+(M+N-1)\E(Y_k)\\
              &= \frac{M}{M+N}+\frac{MN}{M+N}+\frac{N}{M+N}+\frac{MN}{M+N}\\
              &= 1+\frac{2MN}{M+N}
    \end{align*}
\end{example}
\begin{definition}
    For two variables $X$ and $Y$, let $\operatorname{Cov}(X,Y)=\E(X\cdot Y)-\E(X)\cdot\E(Y)=\E((X-\E(X))(Y-\E(Y)))$.
\end{definition}
If $X$ and $Y$ are independent, then $\E(XY)=\E(X)\E(Y)$, so $\operatorname{Cov}(X,Y)=0$.
\begin{example}
    Let $X$ be $-1$, $0$, or $1$ with probability $1/3$ each, and $Y=X^2$.
    Then $\E(X)=0$ and $\E(XY)=\E(X)=0$, so $\operatorname{Cov}(X,Y)=0$.
\end{example}
\begin{proposition}
    \begin{enumerate}
        \item $\Cov(X,X)=\E(X^2)-\E(X)^2=\Var(X)\geq 0$ is almost surely positive definite.
        \item $\Cov(X,Y)=\Cov(Y,X)$
        \item Given variables $X_1,X_2,Y$ and constans $a_1,a_2,c\in\R$,
            \[\Cov(a_1X_1+a_2X_2,Y)=a_1\Cov(X_1,Y)+a_2\Cov(X_2,Y)\]
            so the covariance is bilinear.
        \item $|\Cov(X,Y)|\leq\sigma(X)\sigma(Y)$.
            We often define $\rho(X,Y)=\frac{\Cov(X,Y)}{\sigma(X)\sigma(Y)}$ is the correlation coefficient of $X$ and $Y$.
            Note that $\rho(aX+c,bY+d)=\frac{ab\Cov(X,Y)}{|a||b|\sigma(X)\sigma(Y)}=\rho(X,Y)\sgn(a)\sgn(b)$.
    \end{enumerate}
\end{proposition}
\begin{example}
    Suppose $N$ phones are distributed randomly among $N$ people, where $X$ is the number of matches.
    Write $X=\eta_1+\eta_2+\cdots+\eta_N$, where $\eta_k$ is 1 if person $k$ is matched with her phone, and 0 otherwise.
    Then $\E(X)+N\E(\eta_1)=1$.
    Similarly,
    \begin{align*}
        \Var(X)&=\E(X^2)-\E(X)^2\\
               &= 2-2\sum\limits_{i<j}\E(\eta_i\eta_j)\\
               &= 2-2\binom{N}{2}\frac{1}{N(N-1)}\\
               &= 1
    \end{align*}
    since
    \[\E(\eta_i\eta_j)=\frac{1}{N}\Pr(\eta_j=1|\eta_i=1)=\frac{1}{N}\cdot\frac{1}{N-1}\]
\end{example}
\begin{lemma}
    $\rho(X,Y)\in[-1,1]$ and $\rho(X,Y)=1$ iff $Y=mX+b$ for some $m>0$ (and $-1$ for some $m<0$).
\end{lemma}
\begin{proof}
    Let $\sigma_X,\sigma_Y$ denote the stanard deviations, and $Z=\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y}$.
    We then have
    \begin{align*}
        0\leq\Var(Z) &= \Var\left(\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y}\right)\\
                     &= \Var\left(\frac{X}{\sigma_X}\right)+\Var\left(\frac{Y}{\sigma_Y}\right)+2\Cov\left(\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y}\right)\\
                     &= \frac{\Var X}{\sigma_X^2}+\frac{\Var Y}{\sigma_Y^2}+2\frac{\Cov(X,Y)}{\sigma_X\sigma_Y}\\
                     &= 2(1+\rho(X,Y))
    \end{align*}
    In other words that $1+\delta(X,Y)\geq 0$ if and only $\rho(X,Y)\geq -1$, while $\Var(Z)=0$ if and only if $\rho(X,Y)=-1$.
    Then $\frac{X}{\sigma_X}+\frac{Y}{\sigma_Y}=d$ almost surely, so $Y=mX+b$ for $m<0$.
    The same argument works for $W=\frac{X}{\sigma_X}-\frac{Y}{\sigma_Y}$ in the positive case.
\end{proof}
\begin{example}
    Consider a process with two states and 3 paths.
    From state $0$, we return to state $0$ in 5 minutes with probability $1/3$, in 7 minutes with probability $1/3$, and travel to state 1 in 3 minutes with probability $1/3$.
    What is the expected amount of time to go to state 1 from state 0?

    Let $X$ be a random variable denoting the choices, so $X=1$ if we choose $A$, $2$ if we choose $B$, or 3 if we choose $C$.
    Then
    \[T|X=
        \begin{cases}
            3 &X=1\\
            5+T&X=2\\
            7+T&X=3
        \end{cases}
    \]
    so that
    \[\E(T|X)=
        \begin{cases}
            3 & X=1\\
            5+\E(T) & X=2\\
            7+\E(T) & X=3
        \end{cases}
    \]
    and
    \[\E(T)=\E(\E(T|X))=\frac{1}{3}\cdot 3+\frac{1}{3}(5+\E(T))+\frac{1}{3}(7+\E(T))\]
\end{example}
Suppose $g(x,y)$ is an arbitrary function from $\R^2\to\R$ such that the first moments below are integrable.
Then $\E(g(X,Y))=\E(\E(g(X,Y)|X))$.
Note that if $g(X,Y)=h_1(X)\cdot h_2(Y)$ (i.e. the variables separate), then
\[\E(h_1(X)h_2(Y)|X)=h_1(X)\cdot\E(h_2(Y)|X)\]
\begin{example}
    Suppose $X\sim\UNI[0,1]$ and $Y|X\sim\UNI[0,X]$.
    Then
    \[\Cov(X,Y)=\E(XY)-\E(X)\E(Y)\]
    Note that $\E(X)=1/2$ and $\E(Y|X)=X/2$.
    We can compute
    \begin{align*}\E(Y) &= \E(\E(Y|X))=\E(X/2)=\frac{1}{2}\E(X)=\frac{1}{4}\end{align*}
    and
    \[\E(XY)=\E(\E(XY|X))=\E(X\E(Y|X))=\frac{1}{2}\E(X^2)=\frac{1}{6}\]
\end{example}
\begin{example}[Sum with random number of terms]
    Suppose that the number of customers which enter a store with an hour is a random variable $N$ with $\E(N)=n$ fixed.
    Suppose each customer $k$ spends a random amount $X_k$ with $\(X_k)=\mu$ fixed.
    We assume that the variables $X_1,X_2,\ldots,X_k$ and $N$ are independent.
    Let $Y$ denote the income within the hour, so $Y=X_1+X_2+\cdots+X_N$.
    We then have
    \[\E(Y)=\E(\E(Y|N))=\E(N\mu)=\mu\E(N)=n\cdot\mu\]
\end{example}
Let's compute the conditional variance.
\[\Var(X|Y)=\E(X^2|Y)-(\E(X|Y))^2\]
Taking the expected value, we have
\[\E(\Var(X|Y))=\E(\E(X^2|Y))-\E((\E(X|Y))^2)\]
as well, since $\E(X|Y)$ is a random variable, we have
\[\Var(\E(X|Y))=\E((\E(X|Y))^2)-(\E(\E(X|Y)))^2\]
so that
\begin{align*}
    \E(\Var(X|Y))+\Var(\E(X|Y)) &= \E(\E(X^2|Y))-[\E(\E(X|Y))]^2\\
                                &= \E(X^2)-[\E X]^2\\
                                &= \Var(X)
\end{align*}
\begin{example}
    Suppose the departure time of a train is $T\sim\UNI[0,t]$.
    Passengers board the train according to a Poisson process with intensity $\lambda$.
    Let $X$ denote the number of passengers who take the train.
    What is $\E(X)$, $\Var(X)$?

    We have
    \begin{align*}
        \E(X) &= \E(\E(X|T))=\E(\lambda T)\\
              &= \lambda \E(T)=\frac{\lambda t}{2}
    \end{align*}
    since to compute $\E(X|T=s)$, recall that $X|T=s\sim\Poi(\lambda s)$, so $\E(X|T=s)=\lambda s$.
    As well,
    \begin{align*}
        \Var(X) &= \E(\Var(X|T))+\Var(\E(X|T))\\
                &= \lambda\E(T)+\lambda^2\Var(T)\\
                &= \frac{\lambda t}{2}+\frac{\lambda^2 t^2}{12}
    \end{align*}
\end{example}
Consider one random variable $X$.
How can we replace it with something determinstic?

Suppose we choose the value $c\in\R$ for which $\mu=\E(X)$.
Then
\begin{align*}
    d(0) &= \E((c-X)^2)\\
         &= \E((X-\mu)^2+(\mu-c)^2+2(X-\mu)(\mu-c))\\
         &= \Var(X) + (\mu-c)^2
\end{align*}
which is minimized by $c=\mu$; in other words, $\mu$ minimizes the mean square displacement from $c$, and the mean square displacement is the variance.
If instead we minimize the mean difference, we end up with the median.
\begin{example}
    Suppose $S\sim\mathcal{N}(\mu,\sigma^2)$, and we observe $R|S\sim\mathcal{N}(S,1)$, in other words the signal $S$ with some random noise.
    If a value $r$ is received, what is our guess for the value sent?
    We want $\E(S|R=r)$.

    We want to find
    \[f_{S|R}(s|R=r)=\frac{f(s,r)}{f_R(r)}=\frac{f_{R|S}(r|S=s)f_S(s)}{f_R(r)}=: V\]
\end{example}
\section{Moment Generating Functions}
\begin{definition}
    Given a random variable $X$, the \textbf{moment generating function} of $X$ is
    \[M_X(t)=\E(e^{tX})\]
    Note that such a function may not be finite!
\end{definition}
We have some properties of moment generating functions.
\begin{enumerate}[nolistsep]
    \item $M_X(0)=\E(1)=1$
    \item $M'_X(t)=\E(Xe^{tX})$ so $M'_X(0)=\E(X)$.
    \item In general, $M^{(n)}_X(t)=\E(X^ne^{tX})$ so $M^{(n)}_X(0)=\E(X^n)$.
\end{enumerate}
\begin{example}[Common Moment Generating Functions]
    \begin{enumerate}
        \item Suppose $X\sim\operatorname{Binom}(n,p)$.
            Then
            \begin{align*}
                M_X(t) &= \E(e^{tX})=\sum\limits_{k=0}^ne^{tk}\binom{n}{k}p^k(1=p)^{n-k}\\
                       &= \sum\limits_{k=0}^n \binom{n}{k}(pe^t)^k(1-p)^{n-k}\\
                       &= (pe^t+1-p)^n
            \end{align*}
        \item Suppose $X\sim\Poi(\lambda)$.
            Then
            \begin{align*}
                M_X(t) &= \E(e^tX)\\
                       &= \sum\limits_{k=0}^\infty e^{tk}\frac{\lambda^k}{k!}e^{-\lambda}\\
                       &= e^{-\lambda}\sum\limits_{k=0}^\infty\frac{(\lambda e^t)^k}{k!}\\
                       &= e^{-\lambda}{e^{\lambda e^t}}\\
                       &= e^{\lambda(e^t-1)}
            \end{align*}
        \item Suppose $Z\sim\mathcal{N}(0,1)$.
            Then
            \begin{align*}
                M_Z(t) &= \E(e^{tZ})=\int_{-\infty}^\infty e^{tx}\phi(x)\d{x}\\
                       &= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{tx}e^{-x^2/2}\d{x}\\
                       &= \frac{1}{\sqrt{2\pi}}e^{t^2/2}\int_{-\infty}^\infty e^{-(x-t)^2/2}\d{x}\\
                       &= e^{t^2/2}
            \end{align*}
            and if $X=\sigma Z+\mu$,
            \begin{equation*}
                M_X(t) = \E(e^{tX})=e^{t\mu}\E(e^{t\sigma Z})=e^{t\mu}M_Z(t\sigma)
            \end{equation*}
    \end{enumerate}
\end{example}
Suppose $X$ and $Y$ are indpendent.
Then their moment generating functions are also independent.
This follows since
\[M_{X+Y}(t)=\E(e^{t(X+Y)})=\E(e^{tX}e^{tY})=\E(e^{tX})\E(e^{tY})=M_X(t)M_Y(t)\]
Given two random variables $X_1$ and $X_2$, their joint moment generating function is
\[M:\R^2\to\R\quad M(t_1,t_2)=\E(e^{(t_1X_1+t_2X_2)})\]
\section{Multivariate Normal Distribution}
\begin{definition}
    Given random variables $(X_1,X_2,\ldots,X_n)$ random variables, define the matrix $C$ with $C_{ij}=\Cov(X_i,X_j)$.
    This is a symmetric and positive definite matrix.
To se that it is positive definition, let $a\in\R^n$ be arbitrary, so that
\begin{align*}
    a^TCa &= \sum\limits_{i=1}^n a_i(ca)_i\\
          &= \sum\limits_{i=1}^n\sum\limits_{j=1}^n a_i c_{ij}a_j\\
          &= \sum\limits_{i,j=1}^n a_i a_j\Cov(X_iX_j)\\
          &= \Cov\left(\sum\limits_{i=1}^n a_iX_i,\sum\limits_{j=1}^n a_jX_j\right)\\
          &= \Var\left(\sum\limits_{i=1}^n a_iX_i\right)>0
\end{align*}
so as long as the random variables are linearly independent, then the covariance matrix is positive definitite.

Suppose $(X_1,X_2)$ have joint density $f(x_1,x_2)$.
If $Y=(Y_1,Y_2)=k(X)$ where $k$ is an injective smooth map, the joint density $g(y_1,y_2)$ is given by
\[g(y_1,y_2)\frac{1}{\det(J(k))}f(k^{-1}(y_1,y_2))\]
\begin{definition}
    Given $\mu=(\mu_1,\mu_2,\ldots,\mu_n)\in\R^n$, we say that the random variables $(X_1,X_2,\ldots,X_n)$ are jointly normally distributed if their joint density satisfies
    \[f(x_1,x_2,\ldots,x_n) = \frac{\sqrt{\det C^{-1}}}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}(x-\mu)^TA(x-\mu)\right)\]
    where $C$ is the covariance matrix.
\end{definition}
When $\mu=0$ and $C=I_n$, then
\[f(x)=\left(\frac{1}{\sqrt{2\pi}}\right)^n\exp\left(-\frac{1}{2}x_1^2\right)\exp\left(-\frac{1}{2}x_2^2\right)\cdots\exp\left(-\frac{1}{2}x_n^2\right)\]
in other words that $f(x1,\ldots,x_n)=f(x_1)f(x_2)\cdots f(x_n)$.
\begin{proposition}
    If $(X_1,\ldots,X_n)$ are jointly normal, then there exists an invertible matrix $B$ so that $X=By+\mu$ where $Y=(Y_1,Y_2,\ldots,Y_n)$ are as above.
\end{proposition}
\begin{proof}
    Since $A$ is symmetric and positive definite, let $\lambda_1,\lambda_2,\ldots,\lambda_n$ be the positive eigenvalues and $(u_i)$ be the associated eigenvectors.
    Write $P=(u_1\,\cdots\, u_n)$ is a diagonalizing matrix and $A=PDP^{-1}$.
    Define $B^{-1}=D^{1/2}P^{-1}$ so that $A=(B^{-1})^TB^{-1}$ and $B=PD^{-1/2}$.
    Thus $BB^{T}=PD^{-1/2}D^{-1/2}P^{-1}=A^{-1}+C$.
    We also want $X=BY+\mu$ so $Y=B^{-1}(X-\mu)=k(X)$ so $J=\det(B^{-1})=\frac{1}{\det B}$.

    Now, let's compute the joint density of $Y$.
    Note that $\det(A)=\det((B^{-1})^TB^{-1})=(\det B^{-1})^2$ so $\sqrt{\det A}=\frac{1}{\det B}$.
    As well, $(x-\mu)^TA(x-\mu)=(x-\mu)^T(B^{-1})^TB^{-1}(x-\mu)=y^Ty$.
    We have
    \begin{align*}
        g(y) &= \frac{1}{J}\cdot f(x)\\
             &= \det B\cdot\frac{\sqrt{a}}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}(x-\mu)^TA(x-\mu)\right)\\
             &= \frac{1}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}(y_1^2+\cdots+y_n^2)\right)
    \end{align*}
    as required.

    What is the covariance matrix of $(X_1,X_2,\ldots,X_n)$?
    We have
    \begin{align*}
        \Cov(X_i,X_j) &= \Cov\left(\sum\limits_{k=1}^n B_{ik}Y_k+\mu_i,\sum\limits_{l=1}^n B_{jl}Y_l+\mu_j\right)\\
                      &= \sum\limits_{k,l=1}^n B_{ik}B_{jl}\Cov(Y_k,Y_l)\\
                      &= \sum\limits_{l=1}^n B_{il}B_{jl}\\
                      &= \sum\limits_{l=1}^n B_{il}(B^T)_{lj}\\
                      &= (B\cdot B^T)_{ij}\\
                      &= C_{ij}
    \end{align*}
    In particular, if $C_{ij}=0$ for $i\neq j$, then $A_{ij}$ is diagonal so $f(x)$ is a product and $X_1,\ldots,X_n$ are independent.
    Thus for jointly normally distributed variables, uncorrelated variables are independent.
\end{proof}
When $n=2$, we have
\[C=\begin{pmatrix}\sigma_1^2&\delta\sigma_1\sigma_2\\\delta\sigma_1\sigma_2&\sigma_2^2\end{pmatrix}\]
Thus $\det C=(1-\delta^2)\sigma_1^2\sigma_2^2$, so
\[A=C^{-1}=\frac{1}{1-\delta^2}\begin{pmatrix}\frac{1}{\sigma_1^2} &-\frac{\delta}{\sigma_1\sigma_2}\\-\frac{\delta}{\sigma_1\sigma_2} &\frac{1}{\sigma_2^2}\end{pmatrix}\]
Now if $x=(x_1,x_2)$, then
\[(x-\mu)^TA(x-\mu)=\frac{1}{1-\delta^2}\left(\frac{(x_1-\mu_1)^2}{\sigma_1^2}+\frac{(x_2-\mu_2)^2}{\sigma_2^2}-2\delta\frac{(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1\sigma_2}\right)\]
As well,
\[f_{X_1|X_2}(x_1|X_2=x_2) = \frac{f(x_1,x_2)}{f_{X_2}(x_2)}=C(x_2)\exp\left(-\frac{1}{2}\cdot\frac{1}{(1-\delta^2)\sigma_1^2}(x_1^2-2x_1\mu(x_2))\right)\]
where $\mu(x_2)=\mu_1+\delta\frac{\sigma_1}{\sigma_2}(x_2-\mu_2)$.
Thus $X_1|X_2=x_2\sim\mathcal{N}(\mu(x_2),(1-\delta^2)\sigma_1^2)$.
\section{Common Inequalities}
\begin{proposition}[Markov's Inequality]
    Let $X$ be a non-negative random variable, i.e. $X\geq 0$ almost surely.
    Fix $\mu=\E(X)$; then, for any $x>0$, $\Pr(X\geq a)\leq\mu/a$.
\end{proposition}
\begin{proof}
    Let $\eta_a$ be the indicator for the event $X\geq a$.
    Then $\E(\eta_a)=\Pr(X\geq a)$, while $a\cdot\eta_a\leq X$.
    Then $a\cdot \E(\eta_a)\leq\E(X)$ and we are done.
\end{proof}
\begin{proposition}[Chebyshev's Inequality]
    Let $X$ be an arbitrary random variable with $\mu=\E(X)$ and $\sigma^2=\Var(X)$.
    Then, for any $b>0$, $\Pr(|X-\mu|\geq b)\leq\frac{\sigma^2}{b^2}$.
\end{proposition}
\begin{proof}
    Fix $Y=(X-\mu)^2$, so $Y\geq 0$ and $\E(Y)=\sigma^2$.
    Then by Markov, $\Pr((X-\mu)\geq b)=\Pr(Y\geq b^2)\leq\E(Y)/b^2=\sigma^2/b^2$.
\end{proof}
\begin{proposition}[Weak Law of Large Numbers]
    Let $X_1,X_2,\ldots,X_n$ be an i.i.d. sequence of random variables with $\mu=\E(X_k)$ and $\sigma^2=\Var(X_k)$.
    Then for any $\epsilon>0$, $\Pr\left(\lvert\frac{S_n}{n}-\mu\rvert\geq\epsilon\right)\to 0$ as $n\to\infty$.
\end{proposition}
\begin{proof}
    Note that $\E(S_n)=n\mu$ and $\Var(S_n)=n\sigma^2$ by independence.
    Then
    \begin{equation*}
        \Pr\left(\left\lvert\frac{S_n}{n}-\mu\right\rvert\geq\epsilon\right)=\Pr(|S_n-n\mu|\geq n\epsilon)\leq\frac{\Var(S_n)}{(n\epsilon)^2}=\frac{1}{n}\cdot\frac{\sigma^2}{\epsilon^2}
    \end{equation*}
    which tends to $0$ as $n$ goes to infinity.
\end{proof}
As a special case, let $X_k$ be Bernoulli with probability $p\in(0,1)$, and we get Bernoulli's law of large numbers.

Now, consider $(S_n-n\mu)/n^\alpha$.
Then
\begin{equation*}
    \Pr\left(\left\lvert\frac{S_n-n\mu}{n^\alpha}\right\rvert\geq\epsilon\right)=n^{1-2\alpha}\frac{\sigma^2}{\epsilon^2}\to 0
\end{equation*}
whenever $1/2<\alpha$.
This suggests that the order of the fluctuations is actually $\sqrt{n}$.
\begin{proposition}[Central Limit Theorem]
    Let $X_1,X_2,\ldots$ be iid random variables with $\E(X_1)=\mu$ and $\Var(X_1)=\sigma^2$.
    Then for any $a\in \R$,
    \begin{equation*}
        \Pr\left(\frac{S_n-n\mu}{\sqrt{n}\cdot\sigma}\leq a)\to\Phi(a)
    \end{equation*}
    as $n$ goes to infinity.
\end{proposition}
We will prove the Central Limit Theorem under the additional assumption that $M(t)=\E(e^{tX_k})$ exists for some $t_0>0$.
We also have the following lemma (without proof):
\begin{lemma}
    Let $Y_1,Y_2,\ldots$ be a sequence of random variables with moment generation functions, and if $W$ is another random variable.
    Let $M_n(t)$ be the moment generating function for $Y_n$, and $M_W(t)$ for $W$.
    Then of $M_n(t)\to M_W(t)$ for all $t\in\R$, then $F_n(x)\to F_W(x)$.
\end{lemma}
We now prove the Central Limit Theorem
\begin{proof}
    First suppose $\mu=0$ and $\sigma^2=1$.
    Then
    \begin{equation*}
        \frac{S_n-n\mu}{\sqrt{n}\sigma}=\frac{S_n}{\sqrt{n}}
    \end{equation*}
    Define $\Psi(t)=\log M(t)$.
    Then $\Psi(0)=0$, $\Psi'(0)=\mu=0$, and $\Psi''(t)=\Var x=\sigma^2=1$.
    We want to show that
    \begin{equation*}
        M_{\frac{S_n}{\sqrt{n}}(t)\to M_Z(t)=e^{t^2/2}\Leftrightarrow \Psi_{\frac{S_n}{\sqrt{n}}}(t)\to \Phi_Z(t)=\frac{t^2}{2}
    \end{equation*}
    First note that
    \begin{equation*}
        M_{\frac{S_n}{\sqrt{n}}}(t)=\E\left(e^{t\frac{S_n}{\sqrt{n}}}\right)=M_{S_n}\left(\frac{t}{\sqrt{n}}\right)
    \end{equation*}
    and by independence,
    \begin{equation*}
        M_{S_n(t)}=M_{X_1}(t)M_{X_2}(t)\cdots M_{X_n}(t)=(M(t))^n
    \end{equation*}
    Now $\Psi(s)=\Psi(0)+\Psi'(0)s+\Psi''(0)(0)\frac{s^2}{2}+O(s^3)=\frac{s^2}{2}+O(s^3)$ so that
    \[\Psi_{\frac{S_n}{\sqrt{n}}}(t) = n\Psi\left(\frac{t}{\sqrt{n}}\right)=\frac{t^2}{2}+O\left(\frac{t^3}{\sqrt{n}}\right)\]
    and as $n\to\infty$, this tends to $t^2/2$.

    In general, let $\mu\in\R$, $\sigma^2>0$ be arbitrary.
    Then $X_k^*=\frac{X_k-\mu}{\sigma}$ is an iid sequence and
    \[\frac{S_n-n\mu}{\sqrt{n}\sigma}=\frac{S_n^*}{\sqrt{n}}\]
    and the previous statement applies.
\end{proof}
\begin{example}
    Let $X_1,X_2,\ldots$ be independent measurements of a quantity $\mu$.
    Determine $\delta(n)$ so that $\Pr\left(\left\lvert\frac{S_n}{n}-\mu\right\rvert\leq\delta(n)\right)\geq 0.99$.

    We want
    \begin{align*}
        0.99 &\leq \Pr(|S_n-n\mu|\leq n\delta)\\
             &= \Pr\left(\lvert|\frac{S_n-n\mu}{\sqrt{n}\sigma}\rvert\leq\frac{\sqrt{n}\delta}{\sigma}\right)\\
             &= 2\Phi\left(\frac{\sqrt{n}\delta}{\sigma}\right)-1
    \end{align*}
    We usually do not have $\sigma$!
    Instead, let
    \[\overline{X}=\frac{S_n}{n},\frac{(X_1-\overline{X})^2+\cdots+(X_n-\overline{X})^2}{n-1}=s_n^2\]
    To justify the $n-1$ in the denominator, note that
    Note that $\E[(X_1-X)^2]=\Var(X_1)+\Var(\overline{X})-2\Cov(X_1,\overline{X})$.
    Then $\Var(X_1)=\sigma^2$ so $\Var(\overline{X})=\frac{\sigma^2}{n}$, while $\Cov(X_1,\overline{X})=\Cov(X_1,S_n/n)=\frac{1}{n}\sum\limits_{k=1}^n\Cov(X_1,X_k)=\Var(X_1)/n$.
    Now
    \begin{align*}
        \E((X_1-\overline{X})^2+\cdots+(X_n-\overline{X})^2)&=\E[(X_1-X)^2]+\cdots+\E[(X_n-X)^2]\\
                                                            &= n\left(\sigma^2-\frac{\sigma^2}{n}\right)\\
                                                            &= (n-1)\sigma^2
    \end{align*}
    so that $\E(s_n^2)=\sigma^2$.
\end{example}
Get 48 random real number which is a fixed integer value plus a uniform error on $[-1/2,1/2]$.
They are added up and rounded off to the closest integer $S$.
Let $T$ be the sum of the individual integers.
What is $\Pr(T=S)$?

Note that $S_{48}=\sum\limits_{i=1}^{48}Y_i$ is a sum of independent and identically distributed random variables with mean 0 and variance $48\cdot 1/12=4$.
We have
\begin{align*}
    \Pr(T=S) &= \Pr\left(\left\lvert\sum\limits_{i=1}^{48}Y_i\right\rvert<0.5\right)\\
             &= \Pr(-1/2<S_{48}<1/2)\\
             &= \Pr\left(-1/4<\frac{S_{48}-\E(S_{48})}{\mathbb{D}(S_{48})}<1/4\right)\\
             &= 2\Phi(1/4)-1
\end{align*}
\end{document}
